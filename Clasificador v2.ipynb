{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)\n",
    "\n",
    "archivoTweets = 'tweets_limpios.csv'\n",
    "\n",
    "params = {\n",
    "            \"num_epochs\": 100,\n",
    "            \"batch_size\": 50,\n",
    "            \"filter_sizes\": \"3,4,5\",\n",
    "            \"embedding_dim\": 50,\n",
    "            \"l2_reg_lambda\": 0.0,\n",
    "            \"evaluate_every\": 100,\n",
    "            \"dropout_keep_prob\": 0.6\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_char' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-37547f8bb7be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mstart_char\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_char\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mindex_from\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mindex_from\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start_char' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras._impl.keras.utils.data_utils import get_file\n",
    "path = get_file(\n",
    "      'imdb.npz', origin='https://s3.amazonaws.com/text-datasets/imdb.npz')\n",
    "f = np.load(path)\n",
    "x_train = f['x_train']\n",
    "labels_train = f['y_train']\n",
    "x_test = f['x_test']\n",
    "labels_test = f['y_test']\n",
    "f.close()\n",
    "\n",
    "xs = np.concatenate([x_train, x_test])\n",
    "labels = np.concatenate([labels_train, labels_test])\n",
    "\n",
    "if start_char is not None:\n",
    "    xs = [[start_char] + [w + index_from for w in x] for x in xs]\n",
    "elif index_from:\n",
    "    xs = [[w + index_from for w in x] for x in xs]\n",
    "    \n",
    "xs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path='tweets_limpios.csv',\n",
    "              num_words=None,\n",
    "              skip_top=0,\n",
    "              maxlen=None,\n",
    "              seed=113,\n",
    "              start_char=1,\n",
    "              oov_char=2,\n",
    "              index_from=3):\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['Texto', 'Polaridad']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "    df = df.drop(non_selected, axis=1) # Elimina las columnas innecesarias\n",
    "    df = df.dropna(axis=0, how='any', subset=selected) # Elimina filas con valores null\n",
    "    df = df.reindex(np.random.permutation(df.index)) # Revuelve el conjunto de datos\n",
    "    \n",
    "    labels = sorted(list(set(df[selected[1]].tolist())))\n",
    "    clases = range(0, len(labels))\n",
    "    label_dict = dict(zip(labels, clases))\n",
    "    \n",
    "    xs = df[selected[0]].tolist()   \n",
    "    labels = df[selected[1]].apply(lambda y: label_dict[y]).tolist()\n",
    "\n",
    "  if start_char is not None:\n",
    "    xs = [[start_char] + [w + index_from for w in x] for x in xs]\n",
    "  elif index_from:\n",
    "    xs = [[w + index_from for w in x] for x in xs]\n",
    "\n",
    "  if maxlen:\n",
    "    xs, labels = _remove_long_seq(maxlen, xs, labels)\n",
    "    if not xs:\n",
    "      raise ValueError('After filtering for sequences shorter than maxlen=' +\n",
    "                       str(maxlen) + ', no sequence was kept. '\n",
    "                       'Increase maxlen.')\n",
    "  if not num_words:\n",
    "    num_words = max([max(x) for x in xs])\n",
    "\n",
    "  # by convention, use 2 as OOV word\n",
    "  # reserve 'index_from' (=3 by default) characters:\n",
    "  # 0 (padding), 1 (start), 2 (OOV)\n",
    "  if oov_char is not None:\n",
    "    xs = [\n",
    "        [w if (skip_top <= w < num_words) else oov_char for w in x] for x in xs\n",
    "    ]\n",
    "  else:\n",
    "    xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n",
    "\n",
    "  idx = len(x_train)\n",
    "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
    "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n",
    "\n",
    "  return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 27s 2us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "\n",
    "vocab_size = 5000\n",
    "sentence_size = 200\n",
    "embedding_size = 50\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "# we assign the first indices in the vocabulary to special tokens that we use\n",
    "# for padding, as start token, and for indicating unknown words\n",
    "pad_id = 0\n",
    "start_id = 1\n",
    "oov_id = 2\n",
    "index_offset = 2\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(\n",
    "    num_words=vocab_size, start_char=start_id, oov_char=oov_id,\n",
    "    index_from=index_offset)\n",
    "print(len(y_train), \"train sequences\")\n",
    "print(len(y_test), \"test sequences\")\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train_variable, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "x_test = sequence.pad_sequences(x_test_variable, \n",
    "                                maxlen=sentence_size,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=pad_id)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_variable[:10]\n",
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
