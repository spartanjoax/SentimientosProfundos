{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from Unigrams import UnigramaModeloLenguaje\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)\n",
    "\n",
    "archivoTweets = 'tweets_limpios.csv'\n",
    "\n",
    "params = {\n",
    "            \"num_epochs\": 100,\n",
    "            \"batch_size\": 50,\n",
    "            \"filter_sizes\": \"3,4,5\",\n",
    "            \"embedding_dim\": 50,\n",
    "            \"l2_reg_lambda\": 0.0,\n",
    "            \"evaluate_every\": 100,\n",
    "            \"dropout_keep_prob\": 0.6\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7724\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7d748e63233f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodeloUnigramas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnigramaModeloLenguaje\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mvocabulario\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodeloUnigramas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobtenerDiccionario\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mfrecuencias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodeloUnigramas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobtenerVocabularioFrecuencias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\Unigrams.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oraciones)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpalabra\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiccionario\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiccionario\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpalabra\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiccionario\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiccionario\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiccionarioInvertido\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpalabrasUnicas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrecuenciasUnigramas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(archivoTweets)\n",
    "selected = ['Texto', 'Polaridad']\n",
    "non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "df = df.drop(non_selected, axis=1) # Elimina las columnas innecesarias\n",
    "df = df.dropna(axis=0, how='any', subset=selected) # Elimina filas con valores null\n",
    "df = df.reindex(np.random.permutation(df.index)) # Revuelve el conjunto de datos\n",
    "\n",
    "labels = sorted(list(set(df[selected[1]].tolist())))\n",
    "clases = range(0, len(labels))\n",
    "label_dict = dict(zip(labels, clases))\n",
    "\n",
    "xs = df[selected[0]].tolist()   \n",
    "labels = df[selected[1]].apply(lambda y: label_dict[y]).tolist()\n",
    "print(len(xs))\n",
    "#modeloUnigramas = UnigramaModeloLenguaje(xs)\n",
    "#vocabulario = modeloUnigramas.obtenerDiccionario()\n",
    "#frecuencias = modeloUnigramas.obtenerVocabularioFrecuencias()\n",
    "frecuenciasUnigramas = dict()\n",
    "diccionario = dict()\n",
    "diccionarioInvertido = dict()\n",
    "tamanoCorpus = 0\n",
    "for oracion in xs:\n",
    "    for palabra in oracion.split(' '):\n",
    "        frecuenciasUnigramas[palabra] = frecuenciasUnigramas.get(palabra, 0) + 1\n",
    "        tamanoCorpus += 1\n",
    "dictSort = sorted(frecuenciasUnigramas, key=frecuenciasUnigramas.get, reverse=True)\n",
    "print(dictSort)\n",
    "for palabra in dictSort:\n",
    "    if palabra not in self.diccionario:\n",
    "        self.diccionario[palabra] = len(self.diccionario)\n",
    "for k, v in self.diccionario:\n",
    "    self.diccionarioInvertido[v] = k\n",
    "\n",
    "for x in range(0, 10):\n",
    "    palabra = list(vocabulario.keys())[x][0]\n",
    "    print(palabra)\n",
    "    print(vocabulario[palabra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 23022, 73935, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 25279, 5, 2004, 10471, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 63526, 21145, 1, 49265, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 14030, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 23022, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 23022, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215])\n",
      " list([23777, 39, 81226, 14, 739, 20387, 3428, 44, 74, 32, 1831, 15, 150, 18, 112, 3, 1344, 5, 336, 145, 20, 1, 887, 12, 68, 277, 1189, 403, 34, 119, 282, 36, 167, 5, 393, 154, 39, 2299, 15, 1, 548, 88, 81, 101, 4, 1, 3273, 14, 40, 3, 413, 1200, 134, 8208, 41, 180, 138, 14, 3086, 1, 322, 20, 4930, 28948, 359, 5, 3112, 2128, 1, 20045, 19339, 39, 8208, 45, 3661, 27, 372, 5, 127, 53, 20, 1, 1983, 7, 7, 18, 48, 45, 22, 68, 345, 3, 2131, 5, 409, 20, 1, 1983, 15, 3, 3238, 206, 1, 31645, 22, 277, 66, 36, 3, 341, 1, 719, 729, 3, 3865, 1265, 20, 1, 1510, 3, 1219, 2, 282, 22, 277, 2525, 5, 64, 48, 42, 37, 5, 27, 3273, 12, 6, 23030, 75120, 2034, 7, 7, 3771, 3225, 34, 4186, 34, 378, 14, 12583, 296, 3, 1023, 129, 34, 44, 282, 8, 1, 179, 363, 7067, 5, 94, 3, 2131, 16, 3, 5211, 3005, 15913, 21720, 5, 64, 45, 26, 67, 409, 8, 1, 1983, 15, 3261, 501, 206, 1, 31645, 45, 12583, 2877, 26, 67, 78, 48, 26, 491, 16, 3, 702, 1184, 4, 228, 50, 4505, 1, 43259, 20, 118, 12583, 6, 1373, 20, 1, 887, 16, 3, 20447, 20, 24, 3964, 5, 10455, 24, 172, 844, 118, 26, 188, 1488, 122, 1, 6616, 237, 345, 1, 13891, 32804, 31, 3, 39870, 100, 42, 395, 20, 24, 12130, 118, 12583, 889, 82, 102, 584, 3, 252, 31, 1, 400, 4, 4787, 16974, 1962, 3861, 32, 1230, 3186, 34, 185, 4310, 156, 2325, 38, 341, 2, 38, 9048, 7355, 2231, 4846, 2, 32880, 8938, 2610, 34, 23, 457, 340, 5, 1, 1983, 504, 4355, 12583, 215, 237, 21, 340, 5, 4468, 5996, 34689, 37, 26, 277, 119, 51, 109, 1023, 118, 42, 545, 39, 2814, 513, 39, 27, 553, 7, 7, 134, 1, 116, 2022, 197, 4787, 2, 12583, 283, 1667, 5, 111, 10, 255, 110, 4382, 5, 27, 28, 4, 3771, 12267, 16617, 105, 118, 2597, 5, 109, 3, 209, 9, 284, 3, 4325, 496, 1076, 5, 24, 2761, 154, 138, 14, 7673, 11900, 182, 5276, 39, 20422, 15, 1, 548, 5, 120, 48, 42, 37, 257, 139, 4530, 156, 2325, 9, 1, 372, 248, 39, 20, 1, 82, 505, 228, 3, 376, 2131, 37, 29, 1023, 81, 78, 51, 33, 89, 121, 48, 5, 78, 16, 65, 275, 276, 33, 141, 199, 9, 5, 1, 3273, 302, 4, 769, 9, 37, 17648, 275, 7, 7, 39, 276, 11, 19, 77, 6018, 22, 5, 336, 406])]\n",
      "[[1, 23024, 311, 8, 5, 1071, 211, 11, 2177, 32, 3, 171, 57, 16, 48, 84, 5871, 43, 395, 112, 140, 16, 5361, 60, 4479, 152, 10, 3, 5034, 5950, 484, 71, 7, 263, 14, 23024, 73937, 2005, 8, 75, 2438, 7, 634, 73, 8, 5361, 3, 25281, 7, 2006, 10473, 3, 5943, 1536, 36, 69, 66, 207, 142, 67, 1234, 63528, 21147, 3, 49267, 6, 3, 225, 903, 31, 3026, 71, 6, 3, 5865, 12, 696, 4, 67, 1536, 53, 12, 218, 3, 389, 10, 62, 5, 1474, 3726, 804, 7, 3523, 179, 3, 395, 12, 1240, 14032, 32, 311, 5, 355, 346, 2991, 145, 132, 7, 7806, 30, 6, 128, 5361, 1474, 2377, 7, 23024, 311, 12, 534, 14, 110, 1472, 6, 60, 558, 103, 14, 23024, 311, 8, 229, 4189, 50, 5, 2239, 14, 11, 217], [1, 23779, 41, 81228, 16, 741, 20389, 3430, 46, 76, 34, 1833, 17, 152, 20, 114, 5, 1346, 7, 338, 147, 22, 3, 889, 14, 70, 279, 1191, 405, 36, 121, 284, 38, 169, 7, 395, 156, 41, 2301, 17, 3, 550, 90, 83, 103, 6, 3, 3275, 16, 42, 5, 415, 1202, 136, 8210, 43, 182, 140, 16, 3088, 3, 324, 22, 4932, 28950, 361, 7, 3114, 2130, 3, 20047, 19341, 41, 8210, 47, 3663, 29, 374, 7, 129, 55, 22, 3, 1985, 9, 9, 20, 50, 47, 24, 70, 347, 5, 2133, 7, 411, 22, 3, 1985, 17, 5, 3240, 208, 3, 31647, 24, 279, 68, 38, 5, 343, 3, 721, 731, 5, 3867, 1267, 22, 3, 1512, 5, 1221, 4, 284, 24, 279, 2527, 7, 66, 50, 44, 39, 7, 29, 3275, 14, 8, 23032, 75122, 2036, 9, 9, 3773, 3227, 36, 4188, 36, 380, 16, 12585, 298, 5, 1025, 131, 36, 46, 284, 10, 3, 181, 365, 7069, 7, 96, 5, 2133, 18, 5, 5213, 3007, 15915, 21722, 7, 66, 47, 28, 69, 411, 10, 3, 1985, 17, 3263, 503, 208, 3, 31647, 47, 12585, 2879, 28, 69, 80, 50, 28, 493, 18, 5, 704, 1186, 6, 230, 52, 4507, 3, 43261, 22, 120, 12585, 8, 1375, 22, 3, 889, 18, 5, 20449, 22, 26, 3966, 7, 10457, 26, 174, 846, 120, 28, 190, 1490, 124, 3, 6618, 239, 347, 3, 13893, 32806, 33, 5, 39872, 102, 44, 397, 22, 26, 12132, 120, 12585, 891, 84, 104, 586, 5, 254, 33, 3, 402, 6, 4789, 16976, 1964, 3863, 34, 1232, 3188, 36, 187, 4312, 158, 2327, 40, 343, 4, 40, 9050, 7357, 2233, 4848, 4, 32882, 8940, 2612, 36, 25, 459, 342, 7, 3, 1985, 506, 4357, 12585, 217, 239, 23, 342, 7, 4470, 5998, 34691, 39, 28, 279, 121, 53, 111, 1025, 120, 44, 547, 41, 2816, 515, 41, 29, 555, 9, 9, 136, 3, 118, 2024, 199, 4789, 4, 12585, 285, 1669, 7, 113, 12, 257, 112, 4384, 7, 29, 30, 6, 3773, 12269, 16619, 107, 120, 2599, 7, 111, 5, 211, 11, 286, 5, 4327, 498, 1078, 7, 26, 2763, 156, 140, 16, 7675, 11902, 184, 5278, 41, 20424, 17, 3, 550, 7, 122, 50, 44, 39, 259, 141, 4532, 158, 2327, 11, 3, 374, 250, 41, 22, 3, 84, 507, 230, 5, 378, 2133, 39, 31, 1025, 83, 80, 53, 35, 91, 123, 50, 7, 80, 18, 67, 277, 278, 35, 143, 201, 11, 7, 3, 3275, 304, 6, 771, 11, 39, 17650, 277, 9, 9, 41, 278, 13, 21, 79, 6020, 24, 7, 338, 408]]\n"
     ]
    }
   ],
   "source": [
    "pad_id = 0\n",
    "start_id = 1\n",
    "oov_id = 2\n",
    "index_offset = 2\n",
    "\n",
    "path = get_file(\n",
    "      'imdb.npz', origin='https://s3.amazonaws.com/text-datasets/imdb.npz')\n",
    "f = np.load(path)\n",
    "x_train = f['x_train']\n",
    "labels_train = f['y_train']\n",
    "x_test = f['x_test']\n",
    "labels_test = f['y_test']\n",
    "f.close()\n",
    "\n",
    "xs = np.concatenate([x_train, x_test])\n",
    "labels = np.concatenate([labels_train, labels_test])\n",
    "\n",
    "print(xs[0:2])\n",
    "\n",
    "if start_id is not None:\n",
    "    xs = [[start_id] + [w + index_offset for w in x] for x in xs]\n",
    "elif index_from:\n",
    "    xs = [[w + index_offset for w in x] for x in xs]\n",
    "    \n",
    "print(xs[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementación propia del método load_data de Keras basado en el del IMDB\n",
    "#https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/keras/_impl/keras/datasets/imdb.py\n",
    "from tensorflow.python.keras._impl.keras.utils.data_utils import get_file\n",
    "from tensorflow.python.keras._impl.keras.preprocessing.sequence import _remove_long_seq\n",
    "\n",
    "def load_data(path='tweets_limpios.csv',\n",
    "              num_words=None,\n",
    "              skip_top=0,\n",
    "              maxlen=None,\n",
    "              seed=113,\n",
    "              start_char=1,\n",
    "              oov_char=2,\n",
    "              index_from=3):\n",
    "    \n",
    "    #Carga csv\n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['Texto', 'Polaridad']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "    df = df.drop(non_selected, axis=1) # Elimina las columnas innecesarias\n",
    "    df = df.dropna(axis=0, how='any', subset=selected) # Elimina filas con valores null\n",
    "    df = df.reindex(np.random.permutation(df.index)) # Revuelve el conjunto de datos\n",
    "    \n",
    "    #Convierte las etiquetas a un lista consecutiva iniciando en 0\n",
    "    labels = sorted(list(set(df[selected[1]].tolist())))\n",
    "    clases = range(0, len(labels))\n",
    "    label_dict = dict(zip(labels, clases))\n",
    "    \n",
    "    xs = df[selected[0]].tolist()   \n",
    "    labels = df[selected[1]].apply(lambda y: label_dict[y]).tolist()\n",
    "    \n",
    "    #Carga los unigramas del corpus\n",
    "    modeloUnigramas = UnigramaModeloLenguaje(xs)\n",
    "    vocabulario = modeloUnigramas.obtenerVocabulario()\n",
    "\n",
    "    #Agrega un caracter de inicio de oración y agrega el offset del indice\n",
    "    if start_char is not None:\n",
    "        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n",
    "    elif index_from:\n",
    "        xs = [[w + index_from for w in x] for x in xs]\n",
    "\n",
    "    #Ajusta el tamaño de las oraciones al máximo definido\n",
    "    if maxlen:\n",
    "        xs, labels = _remove_long_seq(maxlen, xs, labels)\n",
    "    if not xs:\n",
    "        raise ValueError('After filtering for sequences shorter than maxlen=' +\n",
    "                       str(maxlen) + ', no sequence was kept. '\n",
    "                       'Increase maxlen.')\n",
    "        \n",
    "    #Obtiene el tamaño del vocabulario si no hay máximo de palabras definido\n",
    "    if not num_words:\n",
    "        num_words = max([max(x) for x in xs])\n",
    "\n",
    "    # by convention, use 2 as OOV word\n",
    "    # reserve 'index_from' (=3 by default) characters:\n",
    "    # 0 (padding), 1 (start), 2 (OOV)\n",
    "    if oov_char is not None:\n",
    "        xs = [\n",
    "            [w if (skip_top <= w < num_words) else oov_char for w in x] for x in xs\n",
    "        ]\n",
    "    else:\n",
    "        xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n",
    "\n",
    "    idx = len(x_train)\n",
    "    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
    "    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 27s 2us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "\n",
    "vocab_size = 5000\n",
    "sentence_size = 200\n",
    "embedding_size = 50\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "# we assign the first indices in the vocabulary to special tokens that we use\n",
    "# for padding, as start token, and for indicating unknown words\n",
    "pad_id = 0\n",
    "start_id = 1\n",
    "oov_id = 2\n",
    "index_offset = 2\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(\n",
    "    num_words=vocab_size, start_char=start_id, oov_char=oov_id,\n",
    "    index_from=index_offset)\n",
    "print(len(y_train), \"train sequences\")\n",
    "print(len(y_test), \"test sequences\")\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train_variable, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "x_test = sequence.pad_sequences(x_test_variable, \n",
    "                                maxlen=sentence_size,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=pad_id)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_variable[:10]\n",
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
