{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primera prueba de vectorización y clasificación básica\n",
    "Se va a probar haciendo una vectorización con bag of words y clasificar usando regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>Polaridad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>salgo de #veotv que día más largoo</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@pauladelasheras no te libraras de ayudar meno...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@marodriguezb gracias mar</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>off pensando en el regalito sinde la que se va...</td>\n",
       "      <td>N+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conozco a alguien q es adicto al drama! ja ja ...</td>\n",
       "      <td>P+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Texto Polaridad\n",
       "0                 salgo de #veotv que día más largoo      NONE\n",
       "1  @pauladelasheras no te libraras de ayudar meno...       NEU\n",
       "2                          @marodriguezb gracias mar         P\n",
       "3  off pensando en el regalito sinde la que se va...        N+\n",
       "4  conozco a alguien q es adicto al drama! ja ja ...        P+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "csv = 'tweets_limpios.csv'\n",
    "#Carga los tweets limpios\n",
    "my_df = pd.read_csv(csv,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se divide el corpus en un 90% para entrenamiento y el 10% restante para pruebas. Se planea usar k-cross fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El conjunto de entrenamiento tiene 6496 registros\n",
      "El conjunto de prueba tiene 722 registros\n"
     ]
    }
   ],
   "source": [
    "x = my_df.Texto\n",
    "y = my_df.Polaridad\n",
    "SEED = 42\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=SEED)\n",
    "print \"El conjunto de entrenamiento tiene {0} registros\".format(len(x_train))\n",
    "print \"El conjunto de prueba tiene {0} registros\".format(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se verifica como quedó la distribución del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento:\n",
      "P+      1497\n",
      "NONE    1346\n",
      "N       1194\n",
      "P       1105\n",
      "N+       759\n",
      "NEU      595\n",
      "Name: Polaridad, dtype: int64\n",
      "\n",
      "Conjunto de prueba:\n",
      "P+      157\n",
      "N       140\n",
      "NONE    135\n",
      "P       129\n",
      "N+       87\n",
      "NEU      74\n",
      "Name: Polaridad, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Imprimir el total de tweets por polaridad del conjunto de entrenamiento\n",
    "print \"Conjunto de entrenamiento:\"\n",
    "print y_train.value_counts()\n",
    "#Imprimir el total de tweets por polaridad del conjunto de prueba\n",
    "print \"\\nConjunto de prueba:\"\n",
    "print y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización con bag of words\n",
    "Se va a hacer la vectorización con bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descargar las stopwords y punkt primero con el downloader de nltk\n",
    "#En la consola de Anaconda, correr: python -m nltk.downloader stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "\n",
    "#Método que realiza el entrenamiento y luego hace pruebas para evaluar la precisión\n",
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    t0 = time()    \n",
    "    #Realiza el entrenamiento\n",
    "    cv = KFold(n_splits=4)\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)    \n",
    "    #Luego del entrenamiento, realice las predicciones\n",
    "    y_pred = sentiment_fit.predict(x_test)    \n",
    "    #Calcula el tiempo que duró\n",
    "    train_test_time = time() - t0\n",
    "    #Obtiene el accuracy de las predicciones\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print \"Exactitud: {0:.4f}%\".format(accuracy*100)\n",
    "    print \"Duración de entrenamiento y pruebas: {0:.2f}s\".format(train_test_time)\n",
    "    print \"-\"*80\n",
    "    return accuracy, train_test_time\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "n_features = np.arange(10000,100001,10000)\n",
    "\n",
    "#Método que revisa la precisión de acuerdo a la cantidad de features definidas\n",
    "def nfeature_accuracy_checker(\n",
    "    vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print \"\\n\"\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print \"Validation result for {} features\".format(n)\n",
    "        nfeature_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_test, y_test)\n",
    "        result.append((n,nfeature_accuracy,tt_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT FOR UNIGRAM WITHOUT STOP WORDS\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "Validation result for 10000 features\n",
      "Exactitud: 43.4903%\n",
      "Duración de entrenamiento y pruebas: 0.64s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 20000 features\n",
      "Exactitud: 43.0748%\n",
      "Duración de entrenamiento y pruebas: 0.77s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 30000 features\n",
      "Exactitud: 43.0748%\n",
      "Duración de entrenamiento y pruebas: 0.72s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 40000 features\n",
      "Exactitud: 43.0748%\n",
      "Duración de entrenamiento y pruebas: 0.75s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 50000 features\n",
      "Exactitud: 43.0748%\n",
      "Duración de entrenamiento y pruebas: 0.80s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 60000 features\n",
      "Exactitud: 43.0748%\n",
      "Duración de entrenamiento y pruebas: 0.72s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 70000 features\n",
      "Exactitud: 43.0748%\n",
      "Duración de entrenamiento y pruebas: 0.78s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 80000 features\n",
      "Exactitud: 43.0748%\n",
      "Duración de entrenamiento y pruebas: 0.82s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 90000 features\n",
      "Exactitud: 43.0748%\n",
      "Duración de entrenamiento y pruebas: 0.82s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 100000 features\n",
      "Exactitud: 43.0748%\n",
      "Duración de entrenamiento y pruebas: 0.75s\n",
      "--------------------------------------------------------------------------------\n",
      "RESULT FOR UNIGRAM WITH STOP WORDS\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "Validation result for 10000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.43s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 20000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.46s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 30000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.43s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 40000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.38s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 50000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.52s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 60000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.41s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 70000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.39s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 80000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.38s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 90000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.46s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 100000 features\n",
      "Exactitud: 39.3352%\n",
      "Duración de entrenamiento y pruebas: 0.50s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Importamos los stopwords\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "print \"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\"\n",
    "feature_result_ug = nfeature_accuracy_checker()\n",
    "print \"RESULT FOR UNIGRAM WITH STOP WORDS\\n\"\n",
    "feature_result_wocsw = nfeature_accuracy_checker(stop_words=spanish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
