{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador basado en CNN + LSTM (GRU)\n",
    "Basado en https://github.com/jiegzhan/multi-class-text-classification-cnn-rnn\n",
    "\n",
    "Analizar esto luego: https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent\n",
    "\n",
    "Esto explica como usa rel pr_curve https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Hacer imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "from text_cnn_rnn import TextCNNRNN\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "archivoTweets = 'tweets_limpios.csv'\n",
    "\n",
    "params = {\n",
    "            \"num_epochs\": 100,\n",
    "            \"batch_size\": 32,\n",
    "            \"filter_sizes\": \"3,4,5\",\n",
    "            \"embedding_dim\": 128,\n",
    "            \"num_filters\": 128,\n",
    "            \"l2_reg_lambda\": 0.1,\n",
    "            \"evaluate_every\": 100,\n",
    "            \"dropout_keep_prob\": 0.5,\n",
    "            \"max_pool_size\": 4,\n",
    "            \"hidden_unit\": 300\n",
    "        }\n",
    "\n",
    "#Cargar stopwords\n",
    "#df = pd.read_csv(\"Stopwords.txt\",header=None)\n",
    "#spanish_stopwords = df[0].values.tolist()\n",
    "#print(\"Stopwords cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método que carga los textos pre-procesados con sus polaridades y los prepara para el entrenamiento\n",
    "def cargar_datos_etiquetas(filename):\n",
    "    \"\"\"Carga texto y polaridad\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['Texto', 'Polaridad']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "    df = df.drop(non_selected, axis=1) # Elimina las columnas innecesarias\n",
    "    df = df.dropna(axis=0, how='any', subset=selected) # Elimina filas con valores null\n",
    "    df = df.reindex(np.random.permutation(df.index)) # Revuelve el conjunto de datos\n",
    "\n",
    "    # Convierte las polaridades en etiquetas One-hot\n",
    "    labels = sorted(list(set(df[selected[1]].tolist())))\n",
    "    one_hot = np.zeros((len(labels), len(labels)), int)\n",
    "    np.fill_diagonal(one_hot, 1)\n",
    "    label_dict = dict(zip(labels, one_hot))\n",
    "\n",
    "    #Crea listas con los textos y las etiquetas en formato one-hot\n",
    "    x_raw = df[selected[0]].tolist()\n",
    "    y_raw = df[selected[1]].apply(lambda y: label_dict[y]).tolist()\n",
    "    return x_raw, y_raw, df, labels\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "def train_cnn():\n",
    "    #Paso 0: Cargar oraciones, etiquetas y parámetros\n",
    "    x_raw, y_raw, df, labels = cargar_datos_etiquetas(archivoTweets)\n",
    "\n",
    "    #Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\n",
    "    max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "    logging.info(\" Oración más larga tiene {} palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\".format(max_document_length))\n",
    "    max_document_length += 5\n",
    "    \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)    \n",
    "    my_file = Path(\"fasttext_vocabulario.dat\")\n",
    "    preentrenado = my_file.is_file()\n",
    "    if preentrenado:\n",
    "        logging.info(\" Cargando vectores generados previamente\")\n",
    "        with open('fasttext_vocabulario.dat', 'rb') as fr:\n",
    "            vocab = pickle.load(fr)\n",
    "        embedding = np.load('fasttext_embeddings.npy')\n",
    "\n",
    "        pretrain = vocab_processor.fit(vocab.keys())\n",
    "        x = np.array(list(vocab_processor.transform(x_raw)))\n",
    "        vocab_size = len(vocab)\n",
    "    else:\n",
    "        logging.info(\" Generando vectores\")\n",
    "        x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "        vocab_size = len(vocab_processor.vocabulary_)\n",
    "        \n",
    "    embedding_size = params['embedding_dim']\n",
    "        \n",
    "    y = np.array(y_raw)\n",
    "\n",
    "    #Paso 2: Divide el dataset original en entrenamiento y prueba\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    #Paso 3: revuelve el dataset de entrenamiento y divide el de entrenamiento en entrenamiento y validación\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    x_shuffled = x_train[shuffle_indices]\n",
    "    y_shuffled = y_train[shuffle_indices]\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x_shuffled, y_shuffled, test_size=0.1)\n",
    "\n",
    "    #Paso 4: guarda las etiquetas en un archivo JSON: labels.json para hacer predicciones luego\n",
    "    with open('labels.json', 'w') as outfile:\n",
    "        json.dump(labels, outfile, indent=4)\n",
    "\n",
    "    logging.info(' x_train: {}, x_dev: {}, x_test: {}'.format(len(x_train), len(x_dev), len(x_test)))\n",
    "    logging.info(' y_train: {}, y_dev: {}, y_test: {}'.format(len(y_train), len(y_dev), len(y_test)))\n",
    "\n",
    "    #Paso 5: Construir el grafo y el objeto CNN\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNNRNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes = y_train.shape[1],\n",
    "                vocab_size=vocab_size,\n",
    "                hidden_unit=params['hidden_unit'],\n",
    "                max_pool_size=params['max_pool_size'],\n",
    "                embedding_size = params['embedding_dim'],\n",
    "                filter_sizes=map(int, params['filter_sizes'].split(\",\")),\n",
    "                num_filters = params['num_filters'],\n",
    "                l2_reg_lambda = params['l2_reg_lambda'],\n",
    "                pre_trained=preentrenado)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"trained_model_\" + timestamp))\n",
    "\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            def real_len(batches):\n",
    "                return [np.ceil(np.argmin(batch + [0]) * 1.0 / params['max_pool_size']) for batch in batches]\n",
    "            \n",
    "            # One training step: train the model with one batch\n",
    "            def train_step(x_batch, y_batch):\n",
    "                if preentrenado:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: params['dropout_keep_prob'],\n",
    "                        cnn.embedding_placeholder: embedding,\n",
    "                        cnn.batch_size: len(x_batch),\n",
    "                        cnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                        cnn.real_len: real_len(x_batch)\n",
    "                    }\n",
    "                else:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: params['dropout_keep_prob'],\n",
    "                        cnn.batch_size: len(x_batch),\n",
    "                        cnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                        cnn.real_len: real_len(x_batch)\n",
    "                    }\n",
    "                _, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "\n",
    "            # One evaluation step: evaluate the model with one batch\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                if preentrenado:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: 1.0,\n",
    "                        cnn.embedding_placeholder: embedding,\n",
    "                        cnn.batch_size: len(x_batch),\n",
    "                        cnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                        cnn.real_len: real_len(x_batch),\n",
    "                    }\n",
    "                else:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: 1.0,\n",
    "                        cnn.batch_size: len(x_batch),\n",
    "                        cnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                        cnn.real_len: real_len(x_batch),\n",
    "                    }\n",
    "                step, loss, acc, num_correct = sess.run([global_step, cnn.loss, cnn.accuracy, cnn.num_correct], feed_dict)\n",
    "                #return num_correct\n",
    "                return acc\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "            \n",
    "            #Inicializa las variables del clasificador\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Training starts here\n",
    "            train_batches = batch_iter(list(zip(x_train, y_train)), params['batch_size'], params['num_epochs'])\n",
    "            best_accuracy, best_at_step = 0, 0\n",
    "\n",
    "            #Paso 6: entrenar el modelo de CNN con x_train y y_train (batch por batch)\n",
    "            logging.info(\" Inicio de entrenamiento\")\n",
    "            for train_batch in train_batches:\n",
    "                x_train_batch, y_train_batch = zip(*train_batch)\n",
    "                train_step(x_train_batch, y_train_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % params['evaluate_every'] == 0:\n",
    "                    logging.info(\" Etapa: {}\".format(current_step))\n",
    "                    #Paso 6.1: evaluar el modelo con x_dev y y_dev                  \n",
    "                    dev_accuracy = dev_step(x_dev, y_dev)\n",
    "                    \n",
    "                    logging.critical(' Exactitud en set de validación: {}'.format(dev_accuracy))\n",
    "\n",
    "                    if dev_accuracy >= best_accuracy:\n",
    "                        #Paso 6.2: Guardar el modelo si es el mejor basado en exactitud en el set de validación\n",
    "                        best_accuracy, best_at_step = dev_accuracy, current_step\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        logging.critical(' Modelo guardado en {} en etapa {}'.format(path, best_at_step))\n",
    "                        logging.critical(' Mejor exactitud es {} en etapa {}'.format(best_accuracy, best_at_step))\n",
    "\n",
    "            #Paso 7: Predecir x_test\n",
    "            test_accuracy = dev_step(x_test, y_test)            \n",
    "            logging.critical(' Exactitud en set de pruebas es {} basado en el último modelo {}'.format(test_accuracy, path))\n",
    "            \n",
    "            checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "            logging.critical(' Cargado el mejor modelo para hacer las pruebas: {}'.format(checkpoint_file))\n",
    "            test_accuracy = dev_step(x_test, y_test)\n",
    "            logging.critical(' Exactitud en set de pruebas es {} basado en el mejor modelo {}'.format(test_accuracy, path))\n",
    "            logging.critical(' Entrenamiento completado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Oración más larga tiene 47 palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\n",
      "INFO:root: Cargando vectores generados previamente\n",
      "INFO:root: x_train: 6255, x_dev: 696, x_test: 773\n",
      "INFO:root: y_train: 6255, y_dev: 696, y_test: 773\n",
      "INFO:root: Inicio de entrenamiento\n",
      "INFO:root: Etapa: 100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.45114943385124207\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528953747\\checkpoints\\model-100 en etapa 100\n",
      "CRITICAL:root: Mejor exactitud es 0.45114943385124207 en etapa 100\n",
      "INFO:root: Etapa: 200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4482758641242981\n",
      "INFO:root: Etapa: 300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4482758641242981\n",
      "INFO:root: Etapa: 400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.45114943385124207\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528953747\\checkpoints\\model-400 en etapa 400\n",
      "CRITICAL:root: Mejor exactitud es 0.45114943385124207 en etapa 400\n",
      "INFO:root: Etapa: 500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.46408045291900635\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528953747\\checkpoints\\model-500 en etapa 500\n",
      "CRITICAL:root: Mejor exactitud es 0.46408045291900635 en etapa 500\n",
      "INFO:root: Etapa: 600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.47270116209983826\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528953747\\checkpoints\\model-600 en etapa 600\n",
      "CRITICAL:root: Mejor exactitud es 0.47270116209983826 en etapa 600\n",
      "INFO:root: Etapa: 700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4655172526836395\n",
      "INFO:root: Etapa: 800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4568965435028076\n",
      "INFO:root: Etapa: 900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.47557470202445984\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528953747\\checkpoints\\model-900 en etapa 900\n",
      "CRITICAL:root: Mejor exactitud es 0.47557470202445984 en etapa 900\n",
      "INFO:root: Etapa: 1000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4698275923728943\n",
      "INFO:root: Etapa: 1100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.46408045291900635\n",
      "INFO:root: Etapa: 1200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4712643623352051\n",
      "INFO:root: Etapa: 1300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.477011501789093\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528953747\\checkpoints\\model-1300 en etapa 1300\n",
      "CRITICAL:root: Mejor exactitud es 0.477011501789093 en etapa 1300\n",
      "INFO:root: Etapa: 1400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4683907926082611\n",
      "INFO:root: Etapa: 1500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4669540226459503\n",
      "INFO:root: Etapa: 1600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.48706895112991333\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528953747\\checkpoints\\model-1600 en etapa 1600\n",
      "CRITICAL:root: Mejor exactitud es 0.48706895112991333 en etapa 1600\n",
      "INFO:root: Etapa: 1700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.47413793206214905\n",
      "INFO:root: Etapa: 1800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4899425208568573\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528953747\\checkpoints\\model-1800 en etapa 1800\n",
      "CRITICAL:root: Mejor exactitud es 0.4899425208568573 en etapa 1800\n",
      "INFO:root: Etapa: 1900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.47270116209983826\n",
      "INFO:root: Etapa: 2000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4698275923728943\n",
      "INFO:root: Etapa: 2100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.48419541120529175\n",
      "INFO:root: Etapa: 2200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4669540226459503\n",
      "INFO:root: Etapa: 2300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4655172526836395\n",
      "INFO:root: Etapa: 2400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.477011501789093\n",
      "INFO:root: Etapa: 2500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.46264368295669556\n",
      "INFO:root: Etapa: 2600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4784482717514038\n",
      "INFO:root: Etapa: 2700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4683907926082611\n",
      "INFO:root: Etapa: 2800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4669540226459503\n",
      "INFO:root: Etapa: 2900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4698275923728943\n",
      "INFO:root: Etapa: 3000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4655172526836395\n",
      "INFO:root: Etapa: 3100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.47557470202445984\n",
      "INFO:root: Etapa: 3200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4597701132297516\n",
      "INFO:root: Etapa: 3300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4885057508945465\n",
      "INFO:root: Etapa: 3400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.479885071516037\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-053d846e69b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-dc536bb423ce>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[0mx_train_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-dc536bb423ce>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m    142\u001b[0m                         \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreal_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                     }\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;31m# One evaluation step: evaluate the model with one batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
