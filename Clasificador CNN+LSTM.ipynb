{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador basado en CNN + LSTM (GRU)\n",
    "Basado en https://github.com/jiegzhan/multi-class-text-classification-cnn-rnn\n",
    "\n",
    "Analizar esto luego: https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent\n",
    "\n",
    "Esto explica como usa rel pr_curve https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Hacer imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "from text_cnn_rnn import TextCNNRNN\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "archivoTweets = 'tweets_limpios.csv'\n",
    "\n",
    "params = {\n",
    "            \"num_epochs\": 100,\n",
    "            \"batch_size\": 32,\n",
    "            \"filter_sizes\": \"3,5,7,9\",\n",
    "            \"embedding_dim\": 128,\n",
    "            \"num_filters\": 128,\n",
    "            \"l2_reg_lambda\": 0.1,\n",
    "            \"evaluate_every\": 100,\n",
    "            \"dropout_keep_prob\": 0.5,\n",
    "            \"max_pool_size\": 4,\n",
    "            \"hidden_unit\": 300\n",
    "        }\n",
    "\n",
    "#Cargar stopwords\n",
    "#df = pd.read_csv(\"Stopwords.txt\",header=None)\n",
    "#spanish_stopwords = df[0].values.tolist()\n",
    "#print(\"Stopwords cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método que carga los textos pre-procesados con sus polaridades y los prepara para el entrenamiento\n",
    "def cargar_datos_etiquetas(filename):\n",
    "    \"\"\"Carga texto y polaridad\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['Texto', 'Polaridad']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "    df = df.drop(non_selected, axis=1) # Elimina las columnas innecesarias\n",
    "    df = df.dropna(axis=0, how='any', subset=selected) # Elimina filas con valores null\n",
    "    df = df.reindex(np.random.permutation(df.index)) # Revuelve el conjunto de datos\n",
    "\n",
    "    # Convierte las polaridades en etiquetas One-hot\n",
    "    labels = sorted(list(set(df[selected[1]].tolist())))\n",
    "    one_hot = np.zeros((len(labels), len(labels)), int)\n",
    "    np.fill_diagonal(one_hot, 1)\n",
    "    label_dict = dict(zip(labels, one_hot))\n",
    "\n",
    "    #Crea listas con los textos y las etiquetas en formato one-hot\n",
    "    x_raw = df[selected[0]].tolist()\n",
    "    y_raw = df[selected[1]].apply(lambda y: label_dict[y]).tolist()\n",
    "    return x_raw, y_raw, df, labels\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "def train_cnn():\n",
    "    #Paso 0: Cargar oraciones, etiquetas y parámetros\n",
    "    x_raw, y_raw, df, labels = cargar_datos_etiquetas(archivoTweets)\n",
    "\n",
    "    #Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\n",
    "    max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "    logging.info(\" Oración más larga tiene {} palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\".format(max_document_length))\n",
    "    max_document_length += 5\n",
    "    \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)    \n",
    "    my_file = Path(\"fasttext_vocabulario.dat\")\n",
    "    preentrenado = my_file.is_file()\n",
    "    if preentrenado:\n",
    "        logging.info(\" Cargando vectores generados previamente\")\n",
    "        with open('fasttext_vocabulario.dat', 'rb') as fr:\n",
    "            vocab = pickle.load(fr)\n",
    "        embedding = np.load('fasttext_embeddings.npy')\n",
    "\n",
    "        pretrain = vocab_processor.fit(vocab.keys())\n",
    "        x = np.array(list(vocab_processor.transform(x_raw)))\n",
    "        vocab_size = len(vocab)\n",
    "    else:\n",
    "        logging.info(\" Generando vectores\")\n",
    "        x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "        vocab_size = len(vocab_processor.vocabulary_)\n",
    "        \n",
    "    embedding_size = params['embedding_dim']\n",
    "        \n",
    "    y = np.array(y_raw)\n",
    "\n",
    "    #Paso 2: Divide el dataset original en entrenamiento y prueba\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    #Paso 3: revuelve el dataset de entrenamiento y divide el de entrenamiento en entrenamiento y validación\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    x_shuffled = x_train[shuffle_indices]\n",
    "    y_shuffled = y_train[shuffle_indices]\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x_shuffled, y_shuffled, test_size=0.1)\n",
    "\n",
    "    #Paso 4: guarda las etiquetas en un archivo JSON: labels.json para hacer predicciones luego\n",
    "    with open('labels.json', 'w') as outfile:\n",
    "        json.dump(labels, outfile, indent=4)\n",
    "\n",
    "    logging.info(' x_train: {}, x_dev: {}, x_test: {}'.format(len(x_train), len(x_dev), len(x_test)))\n",
    "    logging.info(' y_train: {}, y_dev: {}, y_test: {}'.format(len(y_train), len(y_dev), len(y_test)))\n",
    "\n",
    "    #Paso 5: Construir el grafo y el objeto CNN\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNNRNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes = y_train.shape[1],\n",
    "                vocab_size=vocab_size,\n",
    "                hidden_unit=params['hidden_unit'],\n",
    "                max_pool_size=params['max_pool_size'],\n",
    "                embedding_size = params['embedding_dim'],\n",
    "                filter_sizes=map(int, params['filter_sizes'].split(\",\")),\n",
    "                num_filters = params['num_filters'],\n",
    "                l2_reg_lambda = params['l2_reg_lambda'],\n",
    "                pre_trained=preentrenado)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"trained_model_\" + timestamp))\n",
    "\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            def real_len(batches):\n",
    "                return [np.ceil(np.argmin(batch + [0]) * 1.0 / params['max_pool_size']) for batch in batches]\n",
    "            \n",
    "            # One training step: train the model with one batch\n",
    "            def train_step(x_batch, y_batch):\n",
    "                if preentrenado:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: params['dropout_keep_prob'],\n",
    "                        cnn.embedding_placeholder: embedding,\n",
    "                        cnn.batch_size: len(x_batch),\n",
    "                        cnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                        cnn.real_len: real_len(x_batch)\n",
    "                    }\n",
    "                else:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: params['dropout_keep_prob'],\n",
    "                        cnn.batch_size: len(x_batch),\n",
    "                        cnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                        cnn.real_len: real_len(x_batch)\n",
    "                    }\n",
    "                _, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "\n",
    "            # One evaluation step: evaluate the model with one batch\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                if preentrenado:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: 1.0,\n",
    "                        cnn.embedding_placeholder: embedding,\n",
    "                        cnn.batch_size: len(x_batch),\n",
    "                        cnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                        cnn.real_len: real_len(x_batch),\n",
    "                    }\n",
    "                else:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: 1.0,\n",
    "                        cnn.batch_size: len(x_batch),\n",
    "                        cnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                        cnn.real_len: real_len(x_batch),\n",
    "                    }\n",
    "                step, loss, acc, num_correct = sess.run([global_step, cnn.loss, cnn.accuracy, cnn.num_correct], feed_dict)\n",
    "                #return num_correct\n",
    "                return acc\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "            \n",
    "            #Inicializa las variables del clasificador\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Training starts here\n",
    "            train_batches = batch_iter(list(zip(x_train, y_train)), params['batch_size'], params['num_epochs'])\n",
    "            best_accuracy, best_at_step = 0, 0\n",
    "\n",
    "            #Paso 6: entrenar el modelo de CNN con x_train y y_train (batch por batch)\n",
    "            logging.info(\" Inicio de entrenamiento\")\n",
    "            for train_batch in train_batches:\n",
    "                x_train_batch, y_train_batch = zip(*train_batch)\n",
    "                train_step(x_train_batch, y_train_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % params['evaluate_every'] == 0:\n",
    "                    logging.info(\" Etapa: {}\".format(current_step))\n",
    "                    #Paso 6.1: evaluar el modelo con x_dev y y_dev                  \n",
    "                    dev_accuracy = dev_step(x_dev, y_dev)\n",
    "                    \n",
    "                    logging.critical(' Exactitud en set de validación: {}'.format(dev_accuracy))\n",
    "\n",
    "                    if dev_accuracy >= best_accuracy:\n",
    "                        #Paso 6.2: Guardar el modelo si es el mejor basado en exactitud en el set de validación\n",
    "                        best_accuracy, best_at_step = dev_accuracy, current_step\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        logging.critical(' Modelo guardado en {} en etapa {}'.format(path, best_at_step))\n",
    "                        logging.critical(' Mejor exactitud es {} en etapa {}'.format(best_accuracy, best_at_step))\n",
    "\n",
    "            #Paso 7: Predecir x_test\n",
    "            test_accuracy = dev_step(x_test, y_test)            \n",
    "            logging.critical(' Exactitud en set de pruebas es {} basado en el último modelo {}'.format(test_accuracy, path))\n",
    "            \n",
    "            checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "            logging.critical(' Cargado el mejor modelo para hacer las pruebas: {}'.format(checkpoint_file))\n",
    "            test_accuracy = dev_step(x_test, y_test)\n",
    "            logging.critical(' Exactitud en set de pruebas es {} basado en el mejor modelo {}'.format(test_accuracy, path))\n",
    "            logging.critical(' Entrenamiento completado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Oración más larga tiene 47 palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-dc536bb423ce>:51: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-dc536bb423ce>:51: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "INFO:root: Cargando vectores generados previamente\n",
      "C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "INFO:summarizer.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "INFO:root: x_train: 6255, x_dev: 696, x_test: 773\n",
      "INFO:root: y_train: 6255, y_dev: 696, y_test: 773\n",
      "INFO:root: Inicio de entrenamiento\n",
      "INFO:root: Etapa: 100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4295977056026459\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528941367\\checkpoints\\model-100 en etapa 100\n",
      "CRITICAL:root: Mejor exactitud es 0.4295977056026459 en etapa 100\n",
      "INFO:root: Etapa: 200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.45258620381355286\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528941367\\checkpoints\\model-200 en etapa 200\n",
      "CRITICAL:root: Mejor exactitud es 0.45258620381355286 en etapa 200\n",
      "INFO:root: Etapa: 300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4497126340866089\n",
      "INFO:root: Etapa: 400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.45114943385124207\n",
      "INFO:root: Etapa: 500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4712643623352051\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528941367\\checkpoints\\model-500 en etapa 500\n",
      "CRITICAL:root: Mejor exactitud es 0.4712643623352051 en etapa 500\n",
      "INFO:root: Etapa: 600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4655172526836395\n",
      "INFO:root: Etapa: 700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4899425208568573\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528941367\\checkpoints\\model-700 en etapa 700\n",
      "CRITICAL:root: Mejor exactitud es 0.4899425208568573 en etapa 700\n",
      "INFO:root: Etapa: 800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4612068831920624\n",
      "INFO:root: Etapa: 900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.477011501789093\n",
      "INFO:root: Etapa: 1000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4698275923728943\n",
      "INFO:root: Etapa: 1100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.47270116209983826\n",
      "INFO:root: Etapa: 1200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.48275861144065857\n",
      "INFO:root: Etapa: 1300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528941367\\checkpoints\\model-1300 en etapa 1300\n",
      "CRITICAL:root: Mejor exactitud es 0.5 en etapa 1300\n",
      "INFO:root: Etapa: 1400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4698275923728943\n",
      "INFO:root: Etapa: 1500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.49568966031074524\n",
      "INFO:root: Etapa: 1600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.48563218116760254\n",
      "INFO:root: Etapa: 1700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4885057508945465\n",
      "INFO:root: Etapa: 1800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.48419541120529175\n"
     ]
    }
   ],
   "source": [
    "train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
