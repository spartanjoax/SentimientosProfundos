{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador basado en CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'una', 'su', 'al', 'lo', 'como', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosostros', 'vosostras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "#Hacer imports y cargar stopwords y vectores entrenados\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import data_helper\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "import gensim\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "archivoTweets = 'tweets_limpios.csv'\n",
    "\n",
    "#Cargar modelo de vectores\n",
    "archivoModelo = 'Vectores.w2v'\n",
    "modelo = gensim.models.FastText.load(archivoModelo)  # Cargar el modelo de vectores ya entrenado\n",
    "\n",
    "#Cargar stopwords\n",
    "df = pd.read_csv(\"Stopwords.txt\",header=None)\n",
    "spanish_stopwords = df[0].values.tolist()\n",
    "print(spanish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método que carga los textos pre-procesados con sus polaridades y los prepara para el entrenamiento\n",
    "def load_data_and_labels(filename):\n",
    "    \"\"\"Carga texto y polaridad\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['Texto', 'Polaridad']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "    df = df.drop(non_selected, axis=1) # Elimina las columnas innecesarias\n",
    "    df = df.dropna(axis=0, how='any', subset=selected) # Elimina filas con valores null\n",
    "    df = df.reindex(np.random.permutation(df.index)) # Revuelve el conjunto de datos\n",
    "\n",
    "    # Convierte las polaridades en etiquetas One-hot\n",
    "    labels = sorted(list(set(df[selected[1]].tolist())))\n",
    "    one_hot = np.zeros((len(labels), len(labels)), int)\n",
    "    np.fill_diagonal(one_hot, 1)\n",
    "    label_dict = dict(zip(labels, one_hot))\n",
    "\n",
    "    #Crea listas con los textos y las etiquetas en formato one-hot\n",
    "    x_raw = df[selected[0]].tolist()\n",
    "    y_raw = df[selected[1]].apply(lambda y: label_dict[y]).tolist()\n",
    "    return x_raw, y_raw, df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn():\n",
    "    print(\"Paso 0: Cargar oraciones, etiquetas y parámetros\")\n",
    "    x_raw, y_raw, df, labels = load_data_and_labels(archivoTweets)\n",
    "\n",
    "    params = {\n",
    "            \"num_epochs\": 500,\n",
    "            \"batch_size\": 100,\n",
    "            \"num_filters\": 32,\n",
    "            \"filter_sizes\": \"3,4,5\",\n",
    "            \"embedding_dim\": 128,\n",
    "            \"l2_reg_lambda\": 0.0,\n",
    "            \"evaluate_every\": 200,\n",
    "            \"dropout_keep_prob\": 0.5\n",
    "        }\n",
    "\n",
    "    \"\"\"Paso 1: pad each sentence to the same length and map each word to an id\"\"\"\n",
    "    max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "    print(\"Oración más larga tiene {} palabras. Se agregan 10 para tener espacio de manipulación para nuevas oraciones\".format(max_document_length))\n",
    "    logging.info(\"Oración más larga tiene {} palabras. Se agregan 10 para tener espacio de manipulación para nuevas oraciones\".format(max_document_length))\n",
    "    max_document_length += 10\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "    y = np.array(y_raw)\n",
    "\n",
    "    \"\"\"Step 2: split the original dataset into train and test sets\"\"\"\n",
    "    x_, x_test, y_, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    \"\"\"Step 3: shuffle the train set and split the train set into train and dev sets\"\"\"\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_)))\n",
    "    x_shuffled = x_[shuffle_indices]\n",
    "    y_shuffled = y_[shuffle_indices]\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x_shuffled, y_shuffled, test_size=0.1)\n",
    "\n",
    "    \"\"\"Step 4: save the labels into labels.json since predict.py needs it\"\"\"\n",
    "    with open('./labels.json', 'w') as outfile:\n",
    "        json.dump(labels, outfile, indent=4)\n",
    "\n",
    "    logging.info('x_train: {}, x_dev: {}, x_test: {}'.format(len(x_train), len(x_dev), len(x_test)))\n",
    "    logging.info('y_train: {}, y_dev: {}, y_test: {}'.format(len(y_train), len(y_dev), len(y_test)))\n",
    "\n",
    "    \"\"\"Step 5: build a graph and cnn object\"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(modelo.wv.vocab),\n",
    "                embedding_size=params['embedding_dim'],\n",
    "                filter_sizes=list(map(int, params['filter_sizes'].split(\",\"))),\n",
    "                num_filters=params['num_filters'],\n",
    "                l2_reg_lambda=params['l2_reg_lambda'])\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"trained_model_\" + timestamp))\n",
    "\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # One training step: train the model with one batch\n",
    "            def train_step(x_batch, y_batch):\n",
    "                feed_dict = {\n",
    "                    cnn.input_x: x_batch,\n",
    "                    cnn.input_y: y_batch,\n",
    "                    cnn.dropout_keep_prob: params['dropout_keep_prob']}\n",
    "                _, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "\n",
    "            # One evaluation step: evaluate the model with one batch\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0}\n",
    "                step, loss, acc, num_correct = sess.run([global_step, cnn.loss, cnn.accuracy, cnn.num_correct], feed_dict)\n",
    "                return num_correct\n",
    "\n",
    "            # Save the word_to_id map since predict.py needs it\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab.pickle\"))\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Training starts here\n",
    "            train_batches = data_helper.batch_iter(list(zip(x_train, y_train)), params['batch_size'], params['num_epochs'])\n",
    "            best_accuracy, best_at_step = 0, 0\n",
    "\n",
    "            \"\"\"Step 6: train the cnn model with x_train and y_train (batch by batch)\"\"\"\n",
    "            for train_batch in train_batches:\n",
    "                x_train_batch, y_train_batch = zip(*train_batch)\n",
    "                train_step(x_train_batch, y_train_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                \"\"\"Step 6.1: evaluate the model with x_dev and y_dev (batch by batch)\"\"\"\n",
    "                if current_step % params['evaluate_every'] == 0:\n",
    "                    dev_batches = data_helper.batch_iter(list(zip(x_dev, y_dev)), params['batch_size'], 1)\n",
    "                    total_dev_correct = 0\n",
    "                    for dev_batch in dev_batches:\n",
    "                        x_dev_batch, y_dev_batch = zip(*dev_batch)\n",
    "                        num_dev_correct = dev_step(x_dev_batch, y_dev_batch)\n",
    "                        total_dev_correct += num_dev_correct\n",
    "\n",
    "                    dev_accuracy = float(total_dev_correct) / len(y_dev)\n",
    "                    logging.critical('Accuracy on dev set: {}'.format(dev_accuracy))\n",
    "\n",
    "                    \"\"\"Step 6.2: save the model if it is the best based on accuracy on dev set\"\"\"\n",
    "                    if dev_accuracy >= best_accuracy:\n",
    "                        best_accuracy, best_at_step = dev_accuracy, current_step\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        logging.critical('Saved model at {} at step {}'.format(path, best_at_step))\n",
    "                        logging.critical('Best accuracy is {} at step {}'.format(best_accuracy, best_at_step))\n",
    "\n",
    "            \"\"\"Step 7: predict x_test (batch by batch)\"\"\"\n",
    "            test_batches = data_helper.batch_iter(list(zip(x_test, y_test)), params['batch_size'], 1)\n",
    "            total_test_correct = 0\n",
    "            for test_batch in test_batches:\n",
    "                x_test_batch, y_test_batch = zip(*test_batch)\n",
    "                num_test_correct = dev_step(x_test_batch, y_test_batch)\n",
    "                total_test_correct += num_test_correct\n",
    "\n",
    "            test_accuracy = float(total_test_correct) / len(y_test)\n",
    "            logging.critical('Accuracy on test set is {} based on the best model {}'.format(test_accuracy, path))\n",
    "            logging.critical('The training is complete')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # python3 train.py ./data/consumer_complaints.csv.zip ./parameters.json\n",
    "    train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aquí muestra (de sabían xnumx%) hace mes xurlx\n",
      "aquí\n",
      "aquí muestra (de sabía xnumx%) hace mes xurlx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_raw, df, labels = load_data_and_labels(archivoTweets)\n",
    "\n",
    "print(x_raw[0])\n",
    "#lista = [modelo[palabra] if palabra in modelo.wv.vocab else '' for palabra in x_raw[0]]\n",
    "lista = [modelo[palabra] for palabra in x_raw[0].split(' ')] \n",
    "print(modelo.wv.most_similar(positive=[lista[0]], topn=1)[0][0])\n",
    "oracion = ' '.join([modelo.wv.most_similar(positive=[vector], topn=1)[0][0] for vector in lista])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
