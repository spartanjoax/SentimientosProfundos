{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador basado en CNN\n",
    "Basado en https://github.com/satojkovic/cnn-text-classification-tf/tree/use_fasttext y https://github.com/jiegzhan/multi-class-text-classification-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords cargados\n"
     ]
    }
   ],
   "source": [
    "#Hacer imports y cargar stopwords y vectores entrenados\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "archivoTweets = 'tweets_limpios.csv'\n",
    "\n",
    "params = {\n",
    "            \"num_epochs\": 100,\n",
    "            \"batch_size\": 50,\n",
    "            \"num_filters\": 32,\n",
    "            \"filter_sizes\": \"3,4,5\",\n",
    "            \"embedding_dim\": 200,\n",
    "            \"l2_reg_lambda\": 0.0,\n",
    "            \"evaluate_every\": 100,\n",
    "            \"dropout_keep_prob\": 0.5\n",
    "        }\n",
    "\n",
    "#Cargar stopwords\n",
    "df = pd.read_csv(\"Stopwords.txt\",header=None)\n",
    "spanish_stopwords = df[0].values.tolist()\n",
    "print(\"Stopwords cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método que carga los textos pre-procesados con sus polaridades y los prepara para el entrenamiento\n",
    "def cargar_datos_etiquetas(filename):\n",
    "    \"\"\"Carga texto y polaridad\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['Texto', 'Polaridad']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "    df = df.drop(non_selected, axis=1) # Elimina las columnas innecesarias\n",
    "    df = df.dropna(axis=0, how='any', subset=selected) # Elimina filas con valores null\n",
    "    df = df.reindex(np.random.permutation(df.index)) # Revuelve el conjunto de datos\n",
    "\n",
    "    # Convierte las polaridades en etiquetas One-hot\n",
    "    labels = sorted(list(set(df[selected[1]].tolist())))\n",
    "    one_hot = np.zeros((len(labels), len(labels)), int)\n",
    "    np.fill_diagonal(one_hot, 1)\n",
    "    label_dict = dict(zip(labels, one_hot))\n",
    "\n",
    "    #Crea listas con los textos y las etiquetas en formato one-hot\n",
    "    x_raw = df[selected[0]].tolist()\n",
    "    y_raw = df[selected[1]].apply(lambda y: label_dict[y]).tolist()\n",
    "    return x_raw, y_raw, df, labels\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "def train_cnn():\n",
    "    #Paso 0: Cargar oraciones, etiquetas y parámetros\n",
    "    x_raw, y_raw, df, labels = cargar_datos_etiquetas(archivoTweets)\n",
    "\n",
    "    #Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\n",
    "    max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "    logging.info(\" Oración más larga tiene {} palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\".format(max_document_length))\n",
    "    max_document_length += 5\n",
    "    \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)    \n",
    "    my_file = Path(\"fasttext_vocabulario.dat\")\n",
    "    preentrenado = my_file.is_file()\n",
    "    if preentrenado:\n",
    "        logging.info(\" Cargando vectores generados previamente\")\n",
    "        with open('fasttext_vocabulario.dat', 'rb') as fr:\n",
    "            vocab = pickle.load(fr)\n",
    "        embedding = np.load('fasttext_embeddings.npy')\n",
    "\n",
    "        pretrain = vocab_processor.fit(vocab.keys())\n",
    "        x = np.array(list(vocab_processor.transform(x_raw)))\n",
    "        embedding_size = params['embedding_dim']\n",
    "        vocab_size = len(vocab)\n",
    "    else:\n",
    "        logging.info(\" Generando vectores\")\n",
    "        x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "        embedding_size = params['embedding_dim']\n",
    "        vocab_size = len(vocab_processor.vocabulary_)\n",
    "        \n",
    "    y = np.array(y_raw)\n",
    "\n",
    "    #Paso 2: Divide el dataset original en entrenamiento y prueba\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    #Paso 3: revuelve el dataset de entrenamiento y divide el de entrenamiento en entrenamiento y validación\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    x_shuffled = x_train[shuffle_indices]\n",
    "    y_shuffled = y_train[shuffle_indices]\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x_shuffled, y_shuffled, test_size=0.1)\n",
    "\n",
    "    #Paso 4: guarda las etiquetas en un archivo JSON: labels.json para hacer predicciones luego\n",
    "    with open('labels.json', 'w') as outfile:\n",
    "        json.dump(labels, outfile, indent=4)\n",
    "\n",
    "    logging.info(' x_train: {}, x_dev: {}, x_test: {}'.format(len(x_train), len(x_dev), len(x_test)))\n",
    "    logging.info(' y_train: {}, y_dev: {}, y_test: {}'.format(len(y_train), len(y_dev), len(y_test)))\n",
    "\n",
    "    #Paso 5: Construir el grafo y el objeto CNN\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_size=params['embedding_dim'],\n",
    "                filter_sizes=list(map(int, params['filter_sizes'].split(\",\"))),\n",
    "                num_filters=params['num_filters'],\n",
    "                l2_reg_lambda=params['l2_reg_lambda'],\n",
    "                pre_trained=preentrenado)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"trained_model_\" + timestamp))\n",
    "\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            # One training step: train the model with one batch\n",
    "            def train_step(x_batch, y_batch):\n",
    "                if preentrenado:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: params['dropout_keep_prob'],\n",
    "                        cnn.embedding_placeholder: embedding\n",
    "                    }\n",
    "                else:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: params['dropout_keep_prob']\n",
    "                    }\n",
    "                _, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "\n",
    "            # One evaluation step: evaluate the model with one batch\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                if preentrenado:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: 1.0,\n",
    "                        cnn.embedding_placeholder: embedding\n",
    "                    }\n",
    "                else:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: 1.0\n",
    "                    }\n",
    "                step, loss, acc, num_correct = sess.run([global_step, cnn.loss, cnn.accuracy, cnn.num_correct], feed_dict)\n",
    "                #return num_correct\n",
    "                return acc\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "            \n",
    "            #Inicializa las variables del clasificador\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Training starts here\n",
    "            train_batches = batch_iter(list(zip(x_train, y_train)), params['batch_size'], params['num_epochs'])\n",
    "            best_accuracy, best_at_step = 0, 0\n",
    "\n",
    "            #Paso 6: entrenar el modelo de CNN con x_train y y_train (batch por batch)\n",
    "            logging.info(\" Inicio de entrenamiento\")\n",
    "            for train_batch in train_batches:\n",
    "                x_train_batch, y_train_batch = zip(*train_batch)\n",
    "                train_step(x_train_batch, y_train_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % params['evaluate_every'] == 0:\n",
    "                    logging.info(\" Etapa: {}\".format(current_step))\n",
    "                    #Paso 6.1: evaluar el modelo con x_dev y y_dev (batch por batch)\n",
    "                    dev_batches = batch_iter(list(zip(x_dev, y_dev)), params['batch_size'], 1)\n",
    "                    total_dev_correct = 0\n",
    "                    totalBatches = 0\n",
    "                    for dev_batch in dev_batches:\n",
    "                        x_dev_batch, y_dev_batch = zip(*dev_batch)\n",
    "                        total_dev_correct += dev_step(x_dev_batch, y_dev_batch)\n",
    "                        totalBatches += 1\n",
    "\n",
    "                    dev_accuracy = float(total_dev_correct) / totalBatches\n",
    "                    \n",
    "                    #Este es para cuando se usa con la cuenta\n",
    "                    #dev_accuracy = float(total_dev_correct) / len(y_dev)\n",
    "                    \n",
    "                    logging.critical(' Exactitud en set de validación: {}'.format(dev_accuracy))\n",
    "\n",
    "                    if dev_accuracy >= best_accuracy:\n",
    "                        #Paso 6.2: Guardar el modelo si es el mejor basado en exactitud en el set de validación\n",
    "                        best_accuracy, best_at_step = dev_accuracy, current_step\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        logging.critical(' Modelo guardado en {} en etapa {}'.format(path, best_at_step))\n",
    "                        logging.critical(' Mejor exactitud es {} en etapa {}'.format(best_accuracy, best_at_step))\n",
    "\n",
    "            #Paso 7: Predecir x_test (batch por batch)\n",
    "            test_batches = batch_iter(list(zip(x_test, y_test)), params['batch_size'], 1)\n",
    "            total_test_correct = 0\n",
    "            totalBatches = 0\n",
    "            for test_batch in test_batches:\n",
    "                x_test_batch, y_test_batch = zip(*test_batch)\n",
    "                num_test_correct = dev_step(x_test_batch, y_test_batch)\n",
    "                total_test_correct += num_test_correct\n",
    "                totalBatches += 1\n",
    "\n",
    "            test_accuracy = float(total_test_correct) / totalBatches\n",
    "            #test_accuracy = float(total_test_correct) / len(y_test)\n",
    "            logging.critical(' Exactitud en set de pruebas es {} basado en el mejor modelo {}'.format(test_accuracy, path))\n",
    "            logging.critical(' Entrenamiento completado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Oración más larga tiene 28 palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\n",
      "INFO:root: Cargando vectores generados previamente\n",
      "INFO:root: x_train: 5845, x_dev: 650, x_test: 722\n",
      "INFO:root: y_train: 5845, y_dev: 650, y_test: 722\n",
      "INFO:root: Inicio de entrenamiento\n",
      "INFO:root: Etapa: 100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.2938461578809298\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1527901854\\checkpoints\\model-100 en etapa 100\n",
      "CRITICAL:root: Mejor exactitud es 0.2938461578809298 en etapa 100\n",
      "INFO:root: Etapa: 200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3430769294500351\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1527901854\\checkpoints\\model-200 en etapa 200\n",
      "CRITICAL:root: Mejor exactitud es 0.3430769294500351 en etapa 200\n",
      "INFO:root: Etapa: 300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3600000005501967\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1527901854\\checkpoints\\model-300 en etapa 300\n",
      "CRITICAL:root: Mejor exactitud es 0.3600000005501967 en etapa 300\n",
      "INFO:root: Etapa: 400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.34615384615384615\n",
      "INFO:root: Etapa: 500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.35692307582268346\n",
      "INFO:root: Etapa: 600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.36615384542025053\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1527901854\\checkpoints\\model-600 en etapa 600\n",
      "CRITICAL:root: Mejor exactitud es 0.36615384542025053 en etapa 600\n",
      "INFO:root: Etapa: 700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3723076902903043\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1527901854\\checkpoints\\model-700 en etapa 700\n",
      "CRITICAL:root: Mejor exactitud es 0.3723076902903043 en etapa 700\n",
      "INFO:root: Etapa: 800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3892307717066545\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1527901854\\checkpoints\\model-800 en etapa 800\n",
      "CRITICAL:root: Mejor exactitud es 0.3892307717066545 en etapa 800\n",
      "INFO:root: Etapa: 900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3800000044015738\n",
      "INFO:root: Etapa: 1000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.40461538388178897\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1527901854\\checkpoints\\model-1000 en etapa 1000\n",
      "CRITICAL:root: Mejor exactitud es 0.40461538388178897 en etapa 1000\n",
      "INFO:root: Etapa: 1100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3892307671216818\n",
      "INFO:root: Etapa: 1200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.36461538534898025\n",
      "INFO:root: Etapa: 1300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4092307663880862\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1527901854\\checkpoints\\model-1300 en etapa 1300\n",
      "CRITICAL:root: Mejor exactitud es 0.4092307663880862 en etapa 1300\n",
      "INFO:root: Etapa: 1400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.39846154474295103\n",
      "INFO:root: Etapa: 1500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.376923076235331\n",
      "INFO:root: Etapa: 1600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3723076925827907\n",
      "INFO:root: Etapa: 1700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3753846241877629\n",
      "INFO:root: Etapa: 1800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3723076902903043\n",
      "INFO:root: Etapa: 1900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3846153777379256\n",
      "INFO:root: Etapa: 2000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3753846207490334\n",
      "INFO:root: Etapa: 2100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3769230796740605\n",
      "INFO:root: Etapa: 2200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3707692302190341\n",
      "INFO:root: Etapa: 2300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3769230773815742\n",
      "INFO:root: Etapa: 2400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3692307678552774\n",
      "INFO:root: Etapa: 2500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3692307724402501\n",
      "INFO:root: Etapa: 2600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3676923100764935\n",
      "INFO:root: Etapa: 2700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3600000005501967\n",
      "INFO:root: Etapa: 2800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3723076925827907\n",
      "INFO:root: Etapa: 2900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3907692294854384\n",
      "INFO:root: Etapa: 3000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3800000021090874\n",
      "INFO:root: Etapa: 3100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3646153830564939\n",
      "INFO:root: Etapa: 3200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.36615384771273685\n",
      "INFO:root: Etapa: 3300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3876923070504115\n",
      "INFO:root: Etapa: 3400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3846153834691414\n",
      "INFO:root: Etapa: 3500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3846153892003573\n",
      "INFO:root: Etapa: 3600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.37999999752411473\n",
      "INFO:root: Etapa: 3700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38461538232289827\n",
      "INFO:root: Etapa: 3800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3769230773815742\n",
      "INFO:root: Etapa: 3900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3892307694141681\n",
      "INFO:root: Etapa: 4000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.393846149627979\n",
      "INFO:root: Etapa: 4100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3876923116353842\n",
      "INFO:root: Etapa: 4200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38461538690787095\n",
      "INFO:root: Etapa: 4300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3892307717066545\n",
      "INFO:root: Etapa: 4400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3784615363066013\n",
      "INFO:root: Etapa: 4500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3907692294854384\n",
      "INFO:root: Etapa: 4600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3723076925827907\n",
      "INFO:root: Etapa: 4700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.37846153516035813\n",
      "INFO:root: Etapa: 4800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4000000013754918\n",
      "INFO:root: Etapa: 4900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.37384615265406096\n",
      "INFO:root: Etapa: 5000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.39538460969924927\n",
      "INFO:root: Etapa: 5100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3923076941416814\n",
      "INFO:root: Etapa: 5200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38461538690787095\n",
      "INFO:root: Etapa: 5300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.37999999752411473\n",
      "INFO:root: Etapa: 5400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3815384598878714\n",
      "INFO:root: Etapa: 5500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.39230769184919506\n",
      "INFO:root: Etapa: 5600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3707692325115204\n",
      "INFO:root: Etapa: 5700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4030769238105187\n",
      "INFO:root: Etapa: 5800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.40615384853803194\n",
      "INFO:root: Etapa: 5900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3907692340704111\n",
      "INFO:root: Etapa: 6000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3692307678552774\n",
      "INFO:root: Etapa: 6100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3876923116353842\n",
      "INFO:root: Etapa: 6200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3784615397453308\n",
      "INFO:root: Etapa: 6300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38153846447284406\n",
      "INFO:root: Etapa: 6400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3907692294854384\n",
      "INFO:root: Etapa: 6500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3984615390117352\n",
      "INFO:root: Etapa: 6600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.39384615879792434\n",
      "INFO:root: Etapa: 6700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38307692683660066\n",
      "INFO:root: Etapa: 6800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3846153892003573\n",
      "INFO:root: Etapa: 6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root: Exactitud en set de validación: 0.3907692317779248\n",
      "INFO:root: Etapa: 7000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3984615390117352\n",
      "INFO:root: Etapa: 7100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3938461530667085\n",
      "INFO:root: Etapa: 7200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3923076895567087\n",
      "INFO:root: Etapa: 7300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38461538003041196\n",
      "INFO:root: Etapa: 7400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3892307717066545\n",
      "INFO:root: Etapa: 7500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3846153811766551\n",
      "INFO:root: Etapa: 7600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38307692454411435\n",
      "INFO:root: Etapa: 7700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3846153892003573\n",
      "INFO:root: Etapa: 7800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3923076929954382\n",
      "INFO:root: Etapa: 7900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.383076922251628\n",
      "INFO:root: Etapa: 8000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3784615420378171\n",
      "INFO:root: Etapa: 8100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.37692307508908784\n",
      "INFO:root: Etapa: 8200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38307692454411435\n",
      "INFO:root: Etapa: 8300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3676923089302503\n",
      "INFO:root: Etapa: 8400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3707692325115204\n",
      "INFO:root: Etapa: 8500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3723076925827907\n",
      "INFO:root: Etapa: 8600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3799999998166011\n",
      "INFO:root: Etapa: 8700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3753846173103039\n",
      "INFO:root: Etapa: 8800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.37846154089157397\n",
      "INFO:root: Etapa: 8900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3861538446866549\n",
      "INFO:root: Etapa: 9000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3799999998166011\n",
      "INFO:root: Etapa: 9100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.37999999752411473\n",
      "INFO:root: Etapa: 9200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3676923123689798\n",
      "INFO:root: Etapa: 9300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3707692302190341\n",
      "INFO:root: Etapa: 9400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38307692683660066\n",
      "INFO:root: Etapa: 9500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3876923070504115\n",
      "INFO:root: Etapa: 9600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3692307701477638\n",
      "INFO:root: Etapa: 9700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3984615390117352\n",
      "INFO:root: Etapa: 9800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3923076895567087\n",
      "INFO:root: Etapa: 9900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3907692271929521\n",
      "INFO:root: Etapa: 10000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3723076925827907\n",
      "INFO:root: Etapa: 10100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.39230769184919506\n",
      "INFO:root: Etapa: 10200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3815384621803577\n",
      "INFO:root: Etapa: 10300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.37538461501781756\n",
      "INFO:root: Etapa: 10400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38461537888416875\n",
      "INFO:root: Etapa: 10500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38769230246543884\n",
      "INFO:root: Etapa: 10600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3953846108454924\n",
      "INFO:root: Etapa: 10700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3692307724402501\n",
      "INFO:root: Etapa: 10800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3769230773815742\n",
      "INFO:root: Etapa: 10900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3738461538003041\n",
      "INFO:root: Etapa: 11000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38615384697914124\n",
      "INFO:root: Etapa: 11100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.39384615879792434\n",
      "INFO:root: Etapa: 11200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3800000021090874\n",
      "INFO:root: Etapa: 11300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3861538515641139\n",
      "INFO:root: Etapa: 11400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38769230475792515\n",
      "INFO:root: Etapa: 11500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.38307692454411435\n",
      "INFO:root: Etapa: 11600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.35846153933268327\n",
      "INFO:root: Etapa: 11700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.3800000021090874\n",
      "CRITICAL:root: Exactitud en set de pruebas es 0.3235151489575704 basado en el mejor modelo D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1527901854\\checkpoints\\model-1300\n",
      "CRITICAL:root: Entrenamiento completado\n"
     ]
    }
   ],
   "source": [
    "train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:precomputing L2-norms of word weight vectors\n",
      "INFO:gensim.models.keyedvectors:precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7217\n",
      "nueva delegada gobierno andalucía prioriza guadalquivir empleo xurlx\n",
      "1\n",
      "28\n",
      "28\n",
      "28\n",
      "10\n",
      "nueva delegada gobierno andalucía prioriza guadalajara empleo xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_raw, df, labels = load_data_and_labels(archivoTweets)\n",
    "\n",
    "print(len(x_raw))\n",
    "print(x_raw[0])\n",
    "max_document_length = min([len(x.split(' ')) for x in x_raw])\n",
    "print(max_document_length)\n",
    "max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "print(max_document_length)\n",
    "lista = obtener_vectores(x_raw[:10], max_document_length)\n",
    "max_document_length = min([len(x) for x in lista])\n",
    "print(max_document_length)\n",
    "max_document_length = max([len(x) for x in lista])\n",
    "print(max_document_length)\n",
    "oracion = ' '.join([modelo.wv.most_similar(positive=[vector], topn=1)[0][0] for vector in lista[0]])\n",
    "print(len(lista))\n",
    "print(oracion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\n",
      "INFO:root: Oración más larga tiene 28 palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\n",
      "INFO:root: Cargando vectores generados previamente\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7217, 33)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_raw, y_raw, df, labels = cargar_datos_etiquetas(archivoTweets)\n",
    "\n",
    "logging.info(\" Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\")\n",
    "max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "logging.info(\" Oración más larga tiene {} palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\".format(max_document_length))\n",
    "max_document_length += 5\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)    \n",
    "my_file = Path(\"fasttext_vocabulario.dat\")\n",
    "preentrenado = my_file.is_file()\n",
    "if preentrenado:\n",
    "    logging.info(\" Cargando vectores generados previamente\")\n",
    "    with open('fasttext_vocabulario.dat', 'rb') as fr:\n",
    "        vocab = pickle.load(fr)\n",
    "    embedding = np.load('fasttext_embeddings.npy')\n",
    "\n",
    "    pretrain = vocab_processor.fit(vocab.keys())\n",
    "    x = np.array(list(vocab_processor.transform(x_raw)))\n",
    "    embedding_size = params['embedding_dim']\n",
    "    vocab_size = len(vocab)\n",
    "else:\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "    embedding_size = params['embedding_dim']\n",
    "    vocab_size = len(vocab_processor.vocabulary_)\n",
    "    \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
