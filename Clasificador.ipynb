{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador basado en CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de vectores cargado\n",
      "Stopwords cargados\n"
     ]
    }
   ],
   "source": [
    "#Hacer imports y cargar stopwords y vectores entrenados\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import data_helper\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "import gensim\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "archivoTweets = 'tweets_limpios.csv'\n",
    "\n",
    "params = {\n",
    "            \"num_epochs\": 500,\n",
    "            \"batch_size\": 100,\n",
    "            \"num_filters\": 32,\n",
    "            \"filter_sizes\": \"3,4,5\",\n",
    "            \"embedding_dim\": 128,\n",
    "            \"l2_reg_lambda\": 0.0,\n",
    "            \"evaluate_every\": 200,\n",
    "            \"dropout_keep_prob\": 0.5\n",
    "        }\n",
    "\n",
    "#Cargar modelo de vectores\n",
    "archivoModelo = 'Vectores.w2v'\n",
    "modelo = gensim.models.FastText.load(archivoModelo)  # Cargar el modelo de vectores ya entrenado\n",
    "print(\"Modelo de vectores cargado\")\n",
    "\n",
    "#Cargar stopwords\n",
    "df = pd.read_csv(\"Stopwords.txt\",header=None)\n",
    "spanish_stopwords = df[0].values.tolist()\n",
    "print(\"Stopwords cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método que carga los textos pre-procesados con sus polaridades y los prepara para el entrenamiento\n",
    "def cargar_datos_etiquetas(filename):\n",
    "    \"\"\"Carga texto y polaridad\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['Texto', 'Polaridad']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "    df = df.drop(non_selected, axis=1) # Elimina las columnas innecesarias\n",
    "    df = df.dropna(axis=0, how='any', subset=selected) # Elimina filas con valores null\n",
    "    df = df.reindex(np.random.permutation(df.index)) # Revuelve el conjunto de datos\n",
    "\n",
    "    # Convierte las polaridades en etiquetas One-hot\n",
    "    labels = sorted(list(set(df[selected[1]].tolist())))\n",
    "    one_hot = np.zeros((len(labels), len(labels)), int)\n",
    "    np.fill_diagonal(one_hot, 1)\n",
    "    label_dict = dict(zip(labels, one_hot))\n",
    "\n",
    "    #Crea listas con los textos y las etiquetas en formato one-hot\n",
    "    x_raw = df[selected[0]].tolist()\n",
    "    y_raw = df[selected[1]].apply(lambda y: label_dict[y]).tolist()\n",
    "    return x_raw, y_raw, df, labels\n",
    "\n",
    "def obtener_vectores(x_raw, max_document_length):\n",
    "    lista = []\n",
    "    for tweet in x_raw:\n",
    "        tweetVectores = []\n",
    "        for palabra in tweet.split(' '):\n",
    "            try:\n",
    "                tweetVectores.append(modelo.wv.get_vector(palabra))\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if len(tweetVectores) > 0:\n",
    "            if len(tweetVectores) < max_document_length:\n",
    "                for x in range(max_document_length - len(tweetVectores)):\n",
    "                    tweetVectores.append(np.zeros(params['embedding_dim']))\n",
    "            lista.append(tweetVectores)\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn():\n",
    "    print(\"Paso 0: Cargar oraciones, etiquetas y parámetros\")\n",
    "    x_raw, y_raw, df, labels = cargar_datos_etiquetas(archivoTweets)\n",
    "\n",
    "    print(\"Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\")\n",
    "    max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "    print(\"Oración más larga tiene {} palabras. Se agregan 5 para tener espacio de sobra para nuevas oraciones\".format(max_document_length))\n",
    "    logging.info(\"Oración más larga tiene {} palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\".format(max_document_length))\n",
    "    max_document_length += 5\n",
    "    x = np.array(obtener_vectores(x_raw, max_document_length))\n",
    "    y = np.array(y_raw)\n",
    "\n",
    "    print(\"Paso 2: Divide el dataset original en entrenamiento y prueba\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    print(\"Paso 3: revuelve el dataset de entrenamiento y divide el de entrenamiento en entrenamiento y validación\")\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    x_shuffled = x_train[shuffle_indices]\n",
    "    y_shuffled = y_train[shuffle_indices]\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x_shuffled, y_shuffled, test_size=0.1)\n",
    "\n",
    "    print(\"Paso 4: guarda las etiquetas en un archivo JSON: labels.json para hacer predicciones luego\")\n",
    "    with open('labels.json', 'w') as outfile:\n",
    "        json.dump(labels, outfile, indent=4)\n",
    "\n",
    "    logging.info('x_train: {}, x_dev: {}, x_test: {}'.format(len(x_train), len(x_dev), len(x_test)))\n",
    "    logging.info('y_train: {}, y_dev: {}, y_test: {}'.format(len(y_train), len(y_dev), len(y_test)))\n",
    "\n",
    "    print(\"Paso 5: Construir el grafo y el objeto CNN\")\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(modelo.wv.vocab),\n",
    "                embedding_size=params['embedding_dim'],\n",
    "                filter_sizes=list(map(int, params['filter_sizes'].split(\",\"))),\n",
    "                num_filters=params['num_filters'],\n",
    "                l2_reg_lambda=params['l2_reg_lambda'])\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"trained_model_\" + timestamp))\n",
    "\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # One training step: train the model with one batch\n",
    "            def train_step(x_batch, y_batch):\n",
    "                feed_dict = {\n",
    "                    cnn.input_x: x_batch,\n",
    "                    cnn.input_y: y_batch,\n",
    "                    cnn.dropout_keep_prob: params['dropout_keep_prob']}\n",
    "                _, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "\n",
    "            # One evaluation step: evaluate the model with one batch\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0}\n",
    "                step, loss, acc, num_correct = sess.run([global_step, cnn.loss, cnn.accuracy, cnn.num_correct], feed_dict)\n",
    "                return num_correct\n",
    "\n",
    "            #Inicializa las variables del clasificador\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Training starts here\n",
    "            train_batches = data_helper.batch_iter(list(zip(x_train, y_train)), params['batch_size'], params['num_epochs'])\n",
    "            best_accuracy, best_at_step = 0, 0\n",
    "\n",
    "            print(\"Paso 6: entrenar el modelo de CNN con x_train y y_train (batch por batch)\")\n",
    "            for train_batch in train_batches:\n",
    "                x_train_batch, y_train_batch = zip(*train_batch)\n",
    "                train_step(x_train_batch, y_train_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % params['evaluate_every'] == 0:\n",
    "                    print(\"Etapa: {}\".format(current_stop))\n",
    "                    print(\"Paso 6.1: evaluar el modelo con x_dev y y_dev (batch por batch)\")\n",
    "                    dev_batches = data_helper.batch_iter(list(zip(x_dev, y_dev)), params['batch_size'], 1)\n",
    "                    total_dev_correct = 0\n",
    "                    for dev_batch in dev_batches:\n",
    "                        x_dev_batch, y_dev_batch = zip(*dev_batch)\n",
    "                        num_dev_correct = dev_step(x_dev_batch, y_dev_batch)\n",
    "                        total_dev_correct += num_dev_correct\n",
    "\n",
    "                    dev_accuracy = float(total_dev_correct) / len(y_dev)\n",
    "                    logging.critical('Exactitud en set de validación: {}'.format(dev_accuracy))\n",
    "\n",
    "                    if dev_accuracy >= best_accuracy:\n",
    "                        print(\"Paso 6.2: Guardar el modelo si es el mejor basado en exactitud en el set de validación\")\n",
    "                        best_accuracy, best_at_step = dev_accuracy, current_step\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        logging.critical('Modelo guaardado en {} en etapa {}'.format(path, best_at_step))\n",
    "                        logging.critical('Mejor exactitud es {} en etapa {}'.format(best_accuracy, best_at_step))\n",
    "\n",
    "            print(\"Paso 7: Predecir x_test (batch por batch)\")\n",
    "            test_batches = data_helper.batch_iter(list(zip(x_test, y_test)), params['batch_size'], 1)\n",
    "            total_test_correct = 0\n",
    "            for test_batch in test_batches:\n",
    "                x_test_batch, y_test_batch = zip(*test_batch)\n",
    "                num_test_correct = dev_step(x_test_batch, y_test_batch)\n",
    "                total_test_correct += num_test_correct\n",
    "\n",
    "            test_accuracy = float(total_test_correct) / len(y_test)\n",
    "            logging.critical('Exactitud en set de pruebas es {} basado en el mejor modelo {}'.format(test_accuracy, path))\n",
    "            logging.critical('Entrenamiento completado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Oración más larga tiene 28 palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 0: Cargar oraciones, etiquetas y parámetros\n",
      "Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\n",
      "Oración más larga tiene 28 palabras. Se agregan 5 para tener espacio de sobra para nuevas oraciones\n",
      "Paso 2: Divide el dataset original en entrenamiento y prueba\n",
      "Paso 3: revuelve el dataset de entrenamiento y divide el de entrenamiento en entrenamiento y validación\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:x_train: 5845, x_dev: 650, x_test: 722\n",
      "INFO:root:y_train: 5845, y_dev: 650, y_test: 722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 4: guarda las etiquetas en un archivo JSON: labels.json para hacer predicciones luego\n",
      "Paso 5: Construir el grafo y el objeto CNN\n",
      "WARNING:tensorflow:From D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\text_cnn.py:70: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\text_cnn.py:70: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 6: entrenar el modelo de CNN con x_train y y_train (batch por batch)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (100, 33, 128) for Tensor 'input_x:0', which has shape '(?, 33)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-053d846e69b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-159-b241a9aee6a9>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0mx_train_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                 \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m                 \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-159-b241a9aee6a9>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_y\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                     cnn.dropout_keep_prob: params['dropout_keep_prob']}\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;31m# One evaluation step: evaluate the model with one batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1109\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1111\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1112\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (100, 33, 128) for Tensor 'input_x:0', which has shape '(?, 33)'"
     ]
    }
   ],
   "source": [
    "train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7217\n",
      "audiencia vuelve imputar bárcenas merino galeote veamos si cospedal tan buena arquitecta trillo justicia\n",
      "1\n",
      "28\n",
      "28\n",
      "28\n",
      "10\n",
      "audiencia vuelve imputar cenas gabino lanzarote ramos si cospedal tan buena chirigota trillo justicia xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx xurlx\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_raw, df, labels = load_data_and_labels(archivoTweets)\n",
    "\n",
    "print(len(x_raw))\n",
    "print(x_raw[0])\n",
    "max_document_length = min([len(x.split(' ')) for x in x_raw])\n",
    "print(max_document_length)\n",
    "max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "print(max_document_length)\n",
    "lista = obtener_vectores(x_raw[:10], max_document_length)\n",
    "max_document_length = min([len(x) for x in lista])\n",
    "print(max_document_length)\n",
    "max_document_length = max([len(x) for x in lista])\n",
    "print(max_document_length)\n",
    "oracion = ' '.join([modelo.wv.most_similar(positive=[vector], topn=1)[0][0] for vector in lista[0]])\n",
    "print(len(lista))\n",
    "print(oracion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Oración más larga tiene 28 palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\n",
      "Oración más larga tiene 28 palabras. Se agregan 5 para tener espacio de sobra para nuevas oraciones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9260379672050476"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_raw, y_raw, df, labels = cargar_datos_etiquetas(archivoTweets)\n",
    "\n",
    "print(\"Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\")\n",
    "max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "print(\"Oración más larga tiene {} palabras. Se agregan 5 para tener espacio de sobra para nuevas oraciones\".format(max_document_length))\n",
    "logging.info(\"Oración más larga tiene {} palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\".format(max_document_length))\n",
    "max_document_length += 5\n",
    "x = np.array(obtener_vectores(x_raw, max_document_length))\n",
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
