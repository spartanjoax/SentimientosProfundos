{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador basado en CNN\n",
    "Basado en https://github.com/satojkovic/cnn-text-classification-tf/tree/use_fasttext y https://github.com/jiegzhan/multi-class-text-classification-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Hacer imports y cargar stopwords y vectores entrenados\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "archivoTweets = 'tweets_limpios.csv'\n",
    "\n",
    "params = {\n",
    "            \"num_epochs\": 100,\n",
    "            \"batch_size\": 50,\n",
    "            \"num_filters\": 128,\n",
    "            \"filter_sizes\": \"3,4,5\",\n",
    "            \"embedding_dim\": 300,\n",
    "            \"l2_reg_lambda\": 0.0,\n",
    "            \"evaluate_every\": 100,\n",
    "            \"dropout_keep_prob\": 0.6\n",
    "        }\n",
    "\n",
    "#Cargar stopwords\n",
    "#df = pd.read_csv(\"Stopwords.txt\",header=None)\n",
    "#spanish_stopwords = df[0].values.tolist()\n",
    "#print(\"Stopwords cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método que carga los textos pre-procesados con sus polaridades y los prepara para el entrenamiento\n",
    "def cargar_datos_etiquetas(filename):\n",
    "    \"\"\"Carga texto y polaridad\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['Texto', 'Polaridad']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "    df = df.drop(non_selected, axis=1) # Elimina las columnas innecesarias\n",
    "    df = df.dropna(axis=0, how='any', subset=selected) # Elimina filas con valores null\n",
    "    df = df.reindex(np.random.permutation(df.index)) # Revuelve el conjunto de datos\n",
    "\n",
    "    # Convierte las polaridades en etiquetas One-hot\n",
    "    labels = sorted(list(set(df[selected[1]].tolist())))\n",
    "    one_hot = np.zeros((len(labels), len(labels)), int)\n",
    "    np.fill_diagonal(one_hot, 1)\n",
    "    label_dict = dict(zip(labels, one_hot))\n",
    "\n",
    "    #Crea listas con los textos y las etiquetas en formato one-hot\n",
    "    x_raw = df[selected[0]].tolist()\n",
    "    y_raw = df[selected[1]].apply(lambda y: label_dict[y]).tolist()\n",
    "    return x_raw, y_raw, df, labels\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "def train_cnn():\n",
    "    #Paso 0: Cargar oraciones, etiquetas y parámetros\n",
    "    x_raw, y_raw, df, labels = cargar_datos_etiquetas(archivoTweets)\n",
    "\n",
    "    #Paso 1: Obtiene vectores de las palabras y rellena los textos para tener la misma longitud\n",
    "    max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "    logging.info(\" Oración más larga tiene {} palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\".format(max_document_length))\n",
    "    max_document_length += 5\n",
    "    \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)    \n",
    "    my_file = Path(\"fasttext_vocabulario.dat\")\n",
    "    preentrenado = my_file.is_file()\n",
    "    if preentrenado:\n",
    "        logging.info(\" Cargando vectores generados previamente\")\n",
    "        with open('fasttext_vocabulario.dat', 'rb') as fr:\n",
    "            vocab = pickle.load(fr)\n",
    "        embedding = np.load('fasttext_embeddings.npy')\n",
    "\n",
    "        pretrain = vocab_processor.fit(vocab.keys())\n",
    "        x = np.array(list(vocab_processor.transform(x_raw)))\n",
    "        vocab_size = len(vocab)\n",
    "    else:\n",
    "        logging.info(\" Generando vectores\")\n",
    "        x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "        vocab_size = len(vocab_processor.vocabulary_)\n",
    "        \n",
    "    embedding_size = params['embedding_dim']\n",
    "        \n",
    "    y = np.array(y_raw)\n",
    "\n",
    "    #Paso 2: Divide el dataset original en entrenamiento y prueba\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    #Paso 3: revuelve el dataset de entrenamiento y divide el de entrenamiento en entrenamiento y validación\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    x_shuffled = x_train[shuffle_indices]\n",
    "    y_shuffled = y_train[shuffle_indices]\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x_shuffled, y_shuffled, test_size=0.1)\n",
    "\n",
    "    #Paso 4: guarda las etiquetas en un archivo JSON: labels.json para hacer predicciones luego\n",
    "    with open('labels.json', 'w') as outfile:\n",
    "        json.dump(labels, outfile, indent=4)\n",
    "\n",
    "    logging.info(' x_train: {}, x_dev: {}, x_test: {}'.format(len(x_train), len(x_dev), len(x_test)))\n",
    "    logging.info(' y_train: {}, y_dev: {}, y_test: {}'.format(len(y_train), len(y_dev), len(y_test)))\n",
    "\n",
    "    #Paso 5: Construir el grafo y el objeto CNN\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_size=params['embedding_dim'],\n",
    "                filter_sizes=list(map(int, params['filter_sizes'].split(\",\"))),\n",
    "                num_filters=params['num_filters'],\n",
    "                l2_reg_lambda=params['l2_reg_lambda'],\n",
    "                pre_trained=preentrenado)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"trained_model_\" + timestamp))\n",
    "\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            # One training step: train the model with one batch\n",
    "            def train_step(x_batch, y_batch):\n",
    "                if preentrenado:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: params['dropout_keep_prob'],\n",
    "                        cnn.embedding_placeholder: embedding\n",
    "                    }\n",
    "                else:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: params['dropout_keep_prob']\n",
    "                    }\n",
    "                _, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "\n",
    "            # One evaluation step: evaluate the model with one batch\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                if preentrenado:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: 1.0,\n",
    "                        cnn.embedding_placeholder: embedding\n",
    "                    }\n",
    "                else:\n",
    "                    feed_dict = {\n",
    "                        cnn.input_x: x_batch,\n",
    "                        cnn.input_y: y_batch,\n",
    "                        cnn.dropout_keep_prob: 1.0\n",
    "                    }\n",
    "                step, loss, acc, num_correct = sess.run([global_step, cnn.loss, cnn.accuracy, cnn.num_correct], feed_dict)\n",
    "                #return num_correct\n",
    "                return acc\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "            \n",
    "            #Inicializa las variables del clasificador\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Training starts here\n",
    "            train_batches = batch_iter(list(zip(x_train, y_train)), params['batch_size'], params['num_epochs'])\n",
    "            best_accuracy, best_at_step = 0, 0\n",
    "\n",
    "            #Paso 6: entrenar el modelo de CNN con x_train y y_train (batch por batch)\n",
    "            logging.info(\" Inicio de entrenamiento\")\n",
    "            for train_batch in train_batches:\n",
    "                x_train_batch, y_train_batch = zip(*train_batch)\n",
    "                train_step(x_train_batch, y_train_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % params['evaluate_every'] == 0:\n",
    "                    logging.info(\" Etapa: {}\".format(current_step))\n",
    "                    #Paso 6.1: evaluar el modelo con x_dev y y_dev                  \n",
    "                    dev_accuracy = dev_step(x_dev, y_dev)                    \n",
    "                    logging.critical(' Exactitud en set de validación: {}'.format(dev_accuracy))\n",
    "\n",
    "                    if dev_accuracy >= best_accuracy:\n",
    "                        #Paso 6.2: Guardar el modelo si es el mejor basado en exactitud en el set de validación\n",
    "                        best_accuracy, best_at_step = dev_accuracy, current_step\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        logging.critical(' Modelo guardado en {} en etapa {}'.format(path, best_at_step))\n",
    "                        logging.critical(' Mejor exactitud es {} en etapa {}'.format(best_accuracy, best_at_step))\n",
    "\n",
    "            #Paso 7: Predecir x_test\n",
    "            test_accuracy = dev_step(x_test, y_test)\n",
    "            \n",
    "            #test_accuracy = float(total_test_correct) / len(y_test)\n",
    "            logging.critical(' Exactitud en set de pruebas es {} basado en el mejor modelo {}'.format(test_accuracy, path))\n",
    "            logging.critical(' Entrenamiento completado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Oración más larga tiene 28 palabras. Se agregan 5 para tener espacio de manipulación para nuevas oraciones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-042d45a36eac>:51: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-042d45a36eac>:51: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "INFO:root: Cargando vectores generados previamente\n",
      "C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "INFO:summarizer.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joax\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "INFO:root: x_train: 6255, x_dev: 695, x_test: 773\n",
      "INFO:root: y_train: 6255, y_dev: 695, y_test: 773\n",
      "INFO:root: Inicio de entrenamiento\n",
      "INFO:root: Etapa: 100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5079136490821838\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-100 en etapa 100\n",
      "CRITICAL:root: Mejor exactitud es 0.5079136490821838 en etapa 100\n",
      "INFO:root: Etapa: 200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4920863211154938\n",
      "INFO:root: Etapa: 300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-300 en etapa 300\n",
      "CRITICAL:root: Mejor exactitud es 0.5223021507263184 en etapa 300\n",
      "INFO:root: Etapa: 400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5251798629760742\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-400 en etapa 400\n",
      "CRITICAL:root: Mejor exactitud es 0.5251798629760742 en etapa 400\n",
      "INFO:root: Etapa: 500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5136690735816956\n",
      "INFO:root: Etapa: 600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.529496431350708\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-600 en etapa 600\n",
      "CRITICAL:root: Mejor exactitud es 0.529496431350708 en etapa 600\n",
      "INFO:root: Etapa: 700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.529496431350708\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-700 en etapa 700\n",
      "CRITICAL:root: Mejor exactitud es 0.529496431350708 en etapa 700\n",
      "INFO:root: Etapa: 800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5122302174568176\n",
      "INFO:root: Etapa: 900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 1000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 1100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5179855823516846\n",
      "INFO:root: Etapa: 1200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5050359964370728\n",
      "INFO:root: Etapa: 1300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5079136490821838\n",
      "INFO:root: Etapa: 1400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5165467858314514\n",
      "INFO:root: Etapa: 1500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.4978417158126831\n",
      "INFO:root: Etapa: 1600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 1700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5208632946014404\n",
      "INFO:root: Etapa: 1800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 1900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5309352278709412\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-1900 en etapa 1900\n",
      "CRITICAL:root: Mejor exactitud es 0.5309352278709412 en etapa 1900\n",
      "INFO:root: Etapa: 2000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5122302174568176\n",
      "INFO:root: Etapa: 2100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5237410068511963\n",
      "INFO:root: Etapa: 2200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5309352278709412\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-2200 en etapa 2200\n",
      "CRITICAL:root: Mejor exactitud es 0.5309352278709412 en etapa 2200\n",
      "INFO:root: Etapa: 2300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5309352278709412\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-2300 en etapa 2300\n",
      "CRITICAL:root: Mejor exactitud es 0.5309352278709412 en etapa 2300\n",
      "INFO:root: Etapa: 2400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5309352278709412\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-2400 en etapa 2400\n",
      "CRITICAL:root: Mejor exactitud es 0.5309352278709412 en etapa 2400\n",
      "INFO:root: Etapa: 2500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5208632946014404\n",
      "INFO:root: Etapa: 2600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5165467858314514\n",
      "INFO:root: Etapa: 2700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 2800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 2900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5280575752258301\n",
      "INFO:root: Etapa: 3000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5309352278709412\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-3000 en etapa 3000\n",
      "CRITICAL:root: Mejor exactitud es 0.5309352278709412 en etapa 3000\n",
      "INFO:root: Etapa: 3100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.533812940120697\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-3100 en etapa 3100\n",
      "CRITICAL:root: Mejor exactitud es 0.533812940120697 en etapa 3100\n",
      "INFO:root: Etapa: 3200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5366906523704529\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-3200 en etapa 3200\n",
      "CRITICAL:root: Mejor exactitud es 0.5366906523704529 en etapa 3200\n",
      "INFO:root: Etapa: 3300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 3400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5266187191009521\n",
      "INFO:root: Etapa: 3500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5122302174568176\n",
      "INFO:root: Etapa: 3600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5266187191009521\n",
      "INFO:root: Etapa: 3700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5208632946014404\n",
      "INFO:root: Etapa: 3800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5179855823516846\n",
      "INFO:root: Etapa: 3900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5395683646202087\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-3900 en etapa 3900\n",
      "CRITICAL:root: Mejor exactitud es 0.5395683646202087 en etapa 3900\n",
      "INFO:root: Etapa: 4000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5309352278709412\n",
      "INFO:root: Etapa: 4100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.49928057193756104\n",
      "INFO:root: Etapa: 4200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5424460172653198\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-4200 en etapa 4200\n",
      "CRITICAL:root: Mejor exactitud es 0.5424460172653198 en etapa 4200\n",
      "INFO:root: Etapa: 4300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5151079297065735\n",
      "INFO:root: Etapa: 4400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5381295084953308\n",
      "INFO:root: Etapa: 4500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5194244384765625\n",
      "INFO:root: Etapa: 4600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5179855823516846\n",
      "INFO:root: Etapa: 4700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5323740839958191\n",
      "INFO:root: Etapa: 4800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5079136490821838\n",
      "INFO:root: Etapa: 4900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5194244384765625\n",
      "INFO:root: Etapa: 5000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.533812940120697\n",
      "INFO:root: Etapa: 5100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5237410068511963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Etapa: 5200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5266187191009521\n",
      "INFO:root: Etapa: 5300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5079136490821838\n",
      "INFO:root: Etapa: 5400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5251798629760742\n",
      "INFO:root: Etapa: 5500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.48201438784599304\n",
      "INFO:root: Etapa: 5600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5136690735816956\n",
      "INFO:root: Etapa: 5700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5179855823516846\n",
      "INFO:root: Etapa: 5800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.47769784927368164\n",
      "INFO:root: Etapa: 5900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 6000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5107913613319397\n",
      "INFO:root: Etapa: 6100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5280575752258301\n",
      "INFO:root: Etapa: 6200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5021582841873169\n",
      "INFO:root: Etapa: 6300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5122302174568176\n",
      "INFO:root: Etapa: 6400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5208632946014404\n",
      "INFO:root: Etapa: 6500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.49496403336524963\n",
      "INFO:root: Etapa: 6600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.49640288949012756\n",
      "INFO:root: Etapa: 6700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5208632946014404\n",
      "INFO:root: Etapa: 6800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5079136490821838\n",
      "INFO:root: Etapa: 6900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.535251796245575\n",
      "INFO:root: Etapa: 7000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5050359964370728\n",
      "INFO:root: Etapa: 7100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5179855823516846\n",
      "INFO:root: Etapa: 7200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.529496431350708\n",
      "INFO:root: Etapa: 7300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.535251796245575\n",
      "INFO:root: Etapa: 7400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5280575752258301\n",
      "INFO:root: Etapa: 7500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5136690735816956\n",
      "INFO:root: Etapa: 7600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5266187191009521\n",
      "INFO:root: Etapa: 7700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5165467858314514\n",
      "INFO:root: Etapa: 7800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5280575752258301\n",
      "INFO:root: Etapa: 7900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5107913613319397\n",
      "INFO:root: Etapa: 8000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5122302174568176\n",
      "INFO:root: Etapa: 8100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5179855823516846\n",
      "INFO:root: Etapa: 8200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5079136490821838\n",
      "INFO:root: Etapa: 8300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5122302174568176\n",
      "INFO:root: Etapa: 8400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5079136490821838\n",
      "INFO:root: Etapa: 8500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.529496431350708\n",
      "INFO:root: Etapa: 8600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.49064749479293823\n",
      "INFO:root: Etapa: 8700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 8800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5179855823516846\n",
      "INFO:root: Etapa: 8900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5280575752258301\n",
      "INFO:root: Etapa: 9000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 9100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5064747929573059\n",
      "INFO:root: Etapa: 9200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.49928057193756104\n",
      "INFO:root: Etapa: 9300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5151079297065735\n",
      "INFO:root: Etapa: 9400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5122302174568176\n",
      "INFO:root: Etapa: 9500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5381295084953308\n",
      "INFO:root: Etapa: 9600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5309352278709412\n",
      "INFO:root: Etapa: 9700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5035971403121948\n",
      "INFO:root: Etapa: 9800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5266187191009521\n",
      "INFO:root: Etapa: 9900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5323740839958191\n",
      "INFO:root: Etapa: 10000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5223021507263184\n",
      "INFO:root: Etapa: 10100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.529496431350708\n",
      "INFO:root: Etapa: 10200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5366906523704529\n",
      "INFO:root: Etapa: 10300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.529496431350708\n",
      "INFO:root: Etapa: 10400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.533812940120697\n",
      "INFO:root: Etapa: 10500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.533812940120697\n",
      "INFO:root: Etapa: 10600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5323740839958191\n",
      "INFO:root: Etapa: 10700\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5266187191009521\n",
      "INFO:root: Etapa: 10800\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5395683646202087\n",
      "INFO:root: Etapa: 10900\n",
      "CRITICAL:root: Exactitud en set de validación: 0.533812940120697\n",
      "INFO:root: Etapa: 11000\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5208632946014404\n",
      "INFO:root: Etapa: 11100\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5251798629760742\n",
      "INFO:root: Etapa: 11200\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5251798629760742\n",
      "INFO:root: Etapa: 11300\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5525180101394653\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-11300 en etapa 11300\n",
      "CRITICAL:root: Mejor exactitud es 0.5525180101394653 en etapa 11300\n",
      "INFO:root: Etapa: 11400\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5582733750343323\n",
      "CRITICAL:root: Modelo guardado en D:\\respaldo joax\\UCR\\Maestria computacion\\2018-1\\NPL\\Deep learning\\trained_model_1528062338\\checkpoints\\model-11400 en etapa 11400\n",
      "CRITICAL:root: Mejor exactitud es 0.5582733750343323 en etapa 11400\n",
      "INFO:root: Etapa: 11500\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5510791540145874\n",
      "INFO:root: Etapa: 11600\n",
      "CRITICAL:root: Exactitud en set de validación: 0.5381295084953308\n"
     ]
    }
   ],
   "source": [
    "train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
